{"cells":[{"cell_type":"markdown","metadata":{"id":"6j9F-nPGPAew"},"source":["# Classify the sentiments of IMDB reviews"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2549,"status":"ok","timestamp":1711398967139,"user":{"displayName":"Pradeepthi Mallappa","userId":"10327419075559480673"},"user_tz":420},"id":"Cj2XLuMOPAex","outputId":"2b6aafef-64d5-4122-e8e8-6e9733e53121"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"JCKjWTGTPAez","executionInfo":{"status":"ok","timestamp":1711398992019,"user_tz":420,"elapsed":22881,"user":{"displayName":"Pradeepthi Mallappa","userId":"10327419075559480673"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import re\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","from gensim.models import word2vec\n","import gensim.downloader as api\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import SimpleRNN, Dense, Embedding, Dropout\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.callbacks import EarlyStopping\n","from sklearn.preprocessing import LabelEncoder\n","from tensorflow.keras.layers import GRU, Dense, Embedding, Dropout, Bidirectional, LSTM\n","import tensorflow as tf\n","from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n","import warnings\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20978,"status":"ok","timestamp":1711319541583,"user":{"displayName":"Raghavendra K","userId":"12233142523470348717"},"user_tz":420},"id":"0MPbSLmJQ33A","outputId":"cd1f9dbe-d5d5-4420-847c-cfccd564dec2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"eqKMZEIDPAez"},"source":["### 1. Data Loading"]},{"cell_type":"markdown","metadata":{"id":"zY2BLBF9PAez"},"source":["The dataset is from kaggle which is already divided into train, test and validation data set.\n","\n","https://www.kaggle.com/datasets/columbine/imdb-dataset-sentiment-analysis-in-csv-format"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ez4z2lxcPAez"},"outputs":[],"source":["train_data = pd.read_csv(\"/content/drive/MyDrive/NLP/Train.csv\")\n","test_data = pd.read_csv(\"/content/drive/MyDrive/NLP/Test.csv\")\n","valid_data = pd.read_csv(\"/content/drive/MyDrive/NLP/Valid.csv\")"]},{"cell_type":"markdown","metadata":{"id":"A3wA99obPAez"},"source":["### 2. Data Pre-processing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JssMEkQkPAez"},"outputs":[],"source":["def preprocess_text(text):\n","\n","    # Remove HTML tags\n","    text = re.sub('<[^>]*>', '', text)\n","\n","    # Remove non-alphabetic characters and convert to lowercase\n","    text = re.sub('[^a-zA-Z]', ' ', text).lower()\n","\n","    # Tokenize the text\n","    words = word_tokenize(text)\n","\n","    # Remove stopwords from tokenized text -- right now i am not removing stopwords as I dont want to remove words like \"not\" from reviews\n","    # stop_words = set(stopwords.words('english'))\n","    # words = [word for word in words if word not in stop_words]\n","\n","    # Lemmatize the words from tokenized text\n","    lemmatizer = WordNetLemmatizer()\n","    words = [lemmatizer.lemmatize(word) for word in words]\n","\n","    # Combine words back into a single string\n","    preprocessed_text = ' '.join(words)\n","\n","    return preprocessed_text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eMZJgaDLQDPs"},"outputs":[],"source":["train_data['processed_text'] = train_data['text'].apply(preprocess_text)\n","test_data['processed_text'] = test_data['text'].apply(preprocess_text)\n","valid_data['processed_text'] = valid_data['text'].apply(preprocess_text)"]},{"cell_type":"markdown","metadata":{"id":"ouRJ2bf1YUSl"},"source":["### 3. Vectorization (using BoW, TF IDF , Word2Vec) with SVM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_0nonw2QdMS1"},"outputs":[],"source":["X_train = train_data['processed_text']\n","y_train = train_data['label']\n","\n","X_test = test_data['processed_text']\n","y_test = test_data['label']\n","\n","X_valid = valid_data['processed_text']\n","y_valid = valid_data['label']"]},{"cell_type":"markdown","metadata":{"id":"MKK6IUj2b5rH"},"source":["BoW and TF-IDF with SVM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zObq4n7lWQfF"},"outputs":[],"source":["def train_svm_with_representations(train_data, test_data, representation):\n","    if representation == 'bow':\n","        vectorizer = CountVectorizer()\n","    elif representation == 'tfidf':\n","        vectorizer = TfidfVectorizer()\n","    else:\n","        raise ValueError(\"Choose one representation from'bow' or 'tfidf'.\")\n","\n","    X_train = vectorizer.fit_transform(train_data)\n","    X_test = vectorizer.transform(test_data)\n","\n","    clf = SVC()\n","    clf.fit(X_train, y_train)\n","    y_pred = clf.predict(X_test)\n","\n","    return y_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HL4k4iKidxPZ"},"outputs":[],"source":["y_pred_bow = train_svm_with_representations(X_train, X_test, 'bow')\n","accuracy_bow = accuracy_score(y_test, y_pred_bow)\n","print(\"Accuracy of BoW: \", accuracy_bow)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I4MXSuSaeGqe"},"outputs":[],"source":["y_pred_tfidf = train_svm_with_representations(X_train, X_test, 'tfidf')\n","accuracy_tfidf = accuracy_score(y_test, y_pred_tfidf)\n","print(\"Accuracy of TF-IDF: \", accuracy_tfidf)"]},{"cell_type":"markdown","metadata":{"id":"5ukjxc4Rb8_1"},"source":["Word2Vec with SVM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1IffdlCscKB-"},"outputs":[],"source":["def get_word2vec_embeddings(data):\n","    tokenized_sentences = [sentence.split() for sentence in data]\n","    model = word2vec(tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n","    embeddings = np.array([np.mean([model.wv[word] for word in sentence], axis=0) for sentence in tokenized_sentences])\n","\n","    return embeddings\n","\n","def train_svm_with_word2vec(train_data, test_data):\n","    X_train = get_word2vec_embeddings(train_data)\n","    X_test = get_word2vec_embeddings(test_data)\n","\n","    clf = SVC()\n","    clf.fit(X_train, y_train)\n","    y_pred = clf.predict(X_test)\n","\n","    return y_pred"]},{"cell_type":"markdown","metadata":{"id":"u-Sq50kygc4U"},"source":["Word2Vec (using Google News pretrained Word2Vec) with SVM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pBNt6PJ-gcKd"},"outputs":[],"source":["def get_google_word2vec_embeddings(data):\n","    # Load the Google News Word2Vec model\n","    model = api.load(\"word2vec-google-news-300\")\n","\n","    tokenized_sentences = [sentence.split() for sentence in data]\n","    embeddings = []\n","\n","    for sentence in tokenized_sentences:\n","        sentence_embeddings = []\n","        for word in sentence:\n","            if word in model:\n","                sentence_embeddings.append(model[word])\n","        if sentence_embeddings:\n","            embeddings.append(np.mean(sentence_embeddings, axis=0))\n","        else:\n","            embeddings.append(np.zeros(300))\n","\n","    return np.array(embeddings)\n","\n","def train_svm_with_google_word2vec(train_data, test_data):\n","    X_train = get_google_word2vec_embeddings(train_data)\n","    X_test = get_google_word2vec_embeddings(test_data)\n","\n","    clf = SVC()\n","    clf.fit(X_train, y_train)\n","    y_pred = clf.predict(X_test)\n","\n","    return y_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"BzCIgBe0cYxe","outputId":"09811244-852b-47e0-b528-dd1bb9f76639"},"outputs":[{"name":"stdout","output_type":"stream","text":["[==================================================] 100.0% 1662.8/1662.8MB downloaded\n","Accuracy of word2Vec model: 0.8612\n"]}],"source":["# Google News Word2Vec\n","y_pred_google_word2vec = train_svm_with_google_word2vec(X_train, X_test)\n","accuracy_google_word2vec = accuracy_score(y_test, y_pred_google_word2vec)\n","print(\"Accuracy of word2Vec model:\", accuracy_google_word2vec)"]},{"cell_type":"markdown","metadata":{"id":"8fa2hH_Bmjay"},"source":["In addition to using this pre trained model from Google News 300, we can also train our model ourselves by choosing most optimized hyper parameters like window, vector_size, workers, min_count, etc).\n","\n","We can also use any other model other than SVM to see if the accuracy is better."]},{"cell_type":"markdown","metadata":{"id":"U0nTjG1VmEpC"},"source":["### 4. RNN based models (Vanilla RNN, LSTM, GRU, Bi-Directional LSTM)"]},{"cell_type":"markdown","metadata":{"id":"Jfn0ARCWnnV7"},"source":["Vanilla RNN (on processed text) - with early stopping"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":386704,"status":"ok","timestamp":1711092916906,"user":{"displayName":"Pradeepthi Mallappa","userId":"10327419075559480673"},"user_tz":420},"id":"5Bs7GcWalAE_","outputId":"78a0617a-d4e3-4ef5-ad06-793a808ee45c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/25\n","313/313 [==============================] - 37s 111ms/step - loss: 0.4609 - accuracy: 0.7808 - val_loss: 0.3652 - val_accuracy: 0.8470\n","Epoch 2/25\n","313/313 [==============================] - 30s 96ms/step - loss: 0.3049 - accuracy: 0.8795 - val_loss: 0.3476 - val_accuracy: 0.8540\n","Epoch 3/25\n","313/313 [==============================] - 44s 139ms/step - loss: 0.2164 - accuracy: 0.9176 - val_loss: 0.4173 - val_accuracy: 0.8174\n","Epoch 4/25\n","313/313 [==============================] - 29s 92ms/step - loss: 0.1217 - accuracy: 0.9574 - val_loss: 0.5192 - val_accuracy: 0.7996\n","Epoch 5/25\n","313/313 [==============================] - 34s 109ms/step - loss: 0.0662 - accuracy: 0.9777 - val_loss: 0.6435 - val_accuracy: 0.8052\n","Epoch 6/25\n","313/313 [==============================] - 35s 112ms/step - loss: 0.0394 - accuracy: 0.9870 - val_loss: 0.6997 - val_accuracy: 0.8278\n","Epoch 7/25\n","313/313 [==============================] - 28s 88ms/step - loss: 0.0281 - accuracy: 0.9904 - val_loss: 0.8073 - val_accuracy: 0.8394\n","Epoch 8/25\n","313/313 [==============================] - 26s 83ms/step - loss: 0.0609 - accuracy: 0.9777 - val_loss: 0.7186 - val_accuracy: 0.8310\n","Epoch 9/25\n","313/313 [==============================] - 29s 91ms/step - loss: 0.0356 - accuracy: 0.9873 - val_loss: 0.8179 - val_accuracy: 0.8096\n","Epoch 10/25\n","313/313 [==============================] - 27s 85ms/step - loss: 0.0190 - accuracy: 0.9934 - val_loss: 0.9362 - val_accuracy: 0.8168\n","Epoch 11/25\n","313/313 [==============================] - 25s 78ms/step - loss: 0.0202 - accuracy: 0.9937 - val_loss: 1.0491 - val_accuracy: 0.7894\n","Epoch 12/25\n","313/313 [==============================] - 25s 80ms/step - loss: 0.0275 - accuracy: 0.9907 - val_loss: 1.0064 - val_accuracy: 0.8298\n","157/157 [==============================] - 2s 10ms/step - loss: 0.3539 - accuracy: 0.8480\n","Vanila RNN's Test accuracy: 0.8479999899864197\n"]}],"source":["# Tokenize the data\n","max_words = 10000\n","max_len = 100\n","tokenizer = Tokenizer(num_words=max_words)\n","tokenizer.fit_on_texts(X_train)\n","\n","\n","## tokenize\n","X_train = tokenizer.texts_to_sequences(X_train)\n","X_test = tokenizer.texts_to_sequences(X_test)\n","X_valid = tokenizer.texts_to_sequences(X_valid)\n","\n","## padding\n","X_train = pad_sequences(X_train, maxlen=max_len)\n","X_test = pad_sequences(X_test, maxlen=max_len)\n","X_valid = pad_sequences(X_valid, maxlen=max_len)\n","\n","# Create the model\n","model = Sequential()\n","model.add(Embedding(max_words, 128, input_length=max_len))\n","model.add(SimpleRNN(64))\n","model.add(Dropout(0.5))\n","model.add(Dense(2, activation='softmax'))\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Set up early stopping\n","early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","\n","# Train the model\n","history = model.fit(X_train, to_categorical(y_train), validation_data=(X_valid, to_categorical(y_valid)),\n","                    epochs=25, batch_size=128, callbacks=[early_stop])\n","\n","# Evaluate the model\n","loss, accuracy = model.evaluate(X_test, to_categorical(y_test))\n","\n","print(\"Vanila RNN's Test accuracy:\", accuracy)"]},{"cell_type":"markdown","metadata":{"id":"dDZ4hpzpGvLh"},"source":["The epoch stopped at 12 because I have included early stop."]},{"cell_type":"markdown","metadata":{"id":"3kvLK7xgELVI"},"source":["Vanilla RNN (on un processed text) - with early stopping"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":369191,"status":"ok","timestamp":1711093450625,"user":{"displayName":"Pradeepthi Mallappa","userId":"10327419075559480673"},"user_tz":420},"id":"Bog_MvPjD_rX","outputId":"58ee7a8d-efac-478b-abfa-f92ce9e4cc6e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/25\n","313/313 [==============================] - 30s 89ms/step - loss: 0.4851 - accuracy: 0.7517 - val_loss: 0.4010 - val_accuracy: 0.8256\n","Epoch 2/25\n","313/313 [==============================] - 24s 78ms/step - loss: 0.3003 - accuracy: 0.8799 - val_loss: 0.3558 - val_accuracy: 0.8472\n","Epoch 3/25\n","313/313 [==============================] - 35s 112ms/step - loss: 0.1986 - accuracy: 0.9248 - val_loss: 0.4154 - val_accuracy: 0.8548\n","Epoch 4/25\n","313/313 [==============================] - 39s 125ms/step - loss: 0.1045 - accuracy: 0.9637 - val_loss: 0.5588 - val_accuracy: 0.8120\n","Epoch 5/25\n","313/313 [==============================] - 27s 87ms/step - loss: 0.0603 - accuracy: 0.9798 - val_loss: 0.6535 - val_accuracy: 0.8208\n","Epoch 6/25\n","313/313 [==============================] - 29s 92ms/step - loss: 0.0410 - accuracy: 0.9860 - val_loss: 0.7024 - val_accuracy: 0.8178\n","Epoch 7/25\n","313/313 [==============================] - 26s 83ms/step - loss: 0.0329 - accuracy: 0.9894 - val_loss: 0.7649 - val_accuracy: 0.8212\n","Epoch 8/25\n","313/313 [==============================] - 31s 99ms/step - loss: 0.0301 - accuracy: 0.9898 - val_loss: 0.8018 - val_accuracy: 0.8338\n","Epoch 9/25\n","313/313 [==============================] - 26s 84ms/step - loss: 0.0208 - accuracy: 0.9930 - val_loss: 0.8730 - val_accuracy: 0.8416\n","Epoch 10/25\n","313/313 [==============================] - 24s 77ms/step - loss: 0.0373 - accuracy: 0.9869 - val_loss: 0.8997 - val_accuracy: 0.8114\n","Epoch 11/25\n","313/313 [==============================] - 25s 81ms/step - loss: 0.0192 - accuracy: 0.9932 - val_loss: 0.9380 - val_accuracy: 0.8238\n","Epoch 12/25\n","313/313 [==============================] - 26s 82ms/step - loss: 0.0243 - accuracy: 0.9916 - val_loss: 1.0070 - val_accuracy: 0.8278\n","157/157 [==============================] - 1s 9ms/step - loss: 0.3523 - accuracy: 0.8492\n","Vanila RNN's Test accuracy for un processed text: 0.8492000102996826\n"]}],"source":["# Tokenize the data\n","max_words = 10000\n","max_len = 100\n","tokenizer = Tokenizer(num_words=max_words)\n","tokenizer.fit_on_texts(train_data['text'])\n","\n","\n","## tokenize\n","X_train = tokenizer.texts_to_sequences(train_data['text'])\n","X_test = tokenizer.texts_to_sequences(test_data['text'])\n","X_valid = tokenizer.texts_to_sequences(valid_data['text'])\n","\n","## padding\n","X_train = pad_sequences(X_train, maxlen=max_len)\n","X_test = pad_sequences(X_test, maxlen=max_len)\n","X_valid = pad_sequences(X_valid, maxlen=max_len)\n","\n","# Create the model\n","model = Sequential()\n","model.add(Embedding(max_words, 128, input_length=max_len))\n","model.add(SimpleRNN(64))\n","model.add(Dropout(0.5))\n","model.add(Dense(2, activation='softmax'))\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Set up early stopping\n","early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","\n","# Train the model\n","history = model.fit(X_train, to_categorical(y_train), validation_data=(X_valid, to_categorical(y_valid)),\n","                    epochs=25, batch_size=128, callbacks=[early_stop])\n","\n","# Evaluate the model\n","loss, accuracy = model.evaluate(X_test, to_categorical(y_test))\n","\n","print(\"Vanila RNN's Test accuracy for un processed text:\", accuracy)"]},{"cell_type":"markdown","metadata":{"id":"6Uofsbm9ExoW"},"source":["Vanilla RNN has vanishing gradient problem. So to remove this, we use LSTM. The accuracy of Vanilla RNN using un processed text might be more in most cases because the RNN will take care of pre process by ensuring not much information is lost."]},{"cell_type":"markdown","metadata":{"id":"fAw83OkZEnIg"},"source":["LSTM (Long Short Term Memory)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":99769,"status":"ok","timestamp":1711251263464,"user":{"displayName":"Raghavendra K","userId":"12233142523470348717"},"user_tz":420},"id":"JvF6fNJ4Eq3n","outputId":"c31790e8-e283-4c21-d6fd-d2bbab0a5d30"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","313/313 [==============================] - 25s 68ms/step - loss: 0.3976 - accuracy: 0.8181 - val_loss: 0.3364 - val_accuracy: 0.8510\n","Epoch 2/10\n","313/313 [==============================] - 9s 30ms/step - loss: 0.2576 - accuracy: 0.8961 - val_loss: 0.3419 - val_accuracy: 0.8606\n","Epoch 3/10\n","313/313 [==============================] - 5s 17ms/step - loss: 0.2003 - accuracy: 0.9215 - val_loss: 0.3776 - val_accuracy: 0.8558\n","Epoch 4/10\n","313/313 [==============================] - 4s 12ms/step - loss: 0.1580 - accuracy: 0.9393 - val_loss: 0.4316 - val_accuracy: 0.8528\n","Epoch 5/10\n","313/313 [==============================] - 4s 13ms/step - loss: 0.1283 - accuracy: 0.9522 - val_loss: 0.5002 - val_accuracy: 0.8432\n","Epoch 6/10\n","313/313 [==============================] - 3s 10ms/step - loss: 0.1058 - accuracy: 0.9611 - val_loss: 0.5992 - val_accuracy: 0.8402\n","157/157 [==============================] - 1s 5ms/step - loss: 0.3320 - accuracy: 0.8528\n","LSTM test accuracy: 0.8528000116348267\n"]}],"source":["# Tokenize the data\n","max_words = 10000\n","max_len = 100\n","tokenizer = Tokenizer(num_words=max_words)\n","tokenizer.fit_on_texts(train_data['text'])\n","\n","X_train = tokenizer.texts_to_sequences(train_data['text'])\n","X_test = tokenizer.texts_to_sequences(test_data['text'])\n","X_valid = tokenizer.texts_to_sequences(valid_data['text'])\n","\n","X_train = pad_sequences(X_train, maxlen=max_len)\n","X_test = pad_sequences(X_test, maxlen=max_len)\n","X_valid = pad_sequences(X_valid, maxlen=max_len)\n","\n","# Create the model\n","model = Sequential()\n","model.add(Embedding(max_words, 128, input_length=max_len))\n","model.add(LSTM(64))\n","model.add(Dropout(0.5))\n","model.add(Dense(2, activation='softmax'))\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Set up early stopping\n","early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","# Train the model\n","history = model.fit(X_train, to_categorical(y_train), validation_data=(X_valid, to_categorical(y_valid)),\n","                    epochs=10, batch_size=128, callbacks=[early_stop])\n","\n","# Evaluate the model\n","loss, accuracy = model.evaluate(X_test, to_categorical(y_test))\n","\n","print(\"LSTM test accuracy:\", accuracy)"]},{"cell_type":"markdown","metadata":{"id":"k2Hfacs2IIGl"},"source":["Added drop out layer to avoid overfitting"]},{"cell_type":"markdown","metadata":{"id":"QU8dHnhNIMvP"},"source":["GRU Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":98443,"status":"ok","timestamp":1711251361883,"user":{"displayName":"Raghavendra K","userId":"12233142523470348717"},"user_tz":420},"id":"lLoCLFcqICwj","outputId":"7ca12b17-cb87-456d-c66f-b114bb6081f5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/25\n","313/313 [==============================] - 23s 68ms/step - loss: 0.4233 - accuracy: 0.7925 - val_loss: 0.3312 - val_accuracy: 0.8576\n","Epoch 2/25\n","313/313 [==============================] - 7s 23ms/step - loss: 0.2631 - accuracy: 0.8939 - val_loss: 0.3139 - val_accuracy: 0.8686\n","Epoch 3/25\n","313/313 [==============================] - 5s 17ms/step - loss: 0.2098 - accuracy: 0.9183 - val_loss: 0.3355 - val_accuracy: 0.8698\n","Epoch 4/25\n","313/313 [==============================] - 3s 11ms/step - loss: 0.1586 - accuracy: 0.9406 - val_loss: 0.3726 - val_accuracy: 0.8604\n","Epoch 5/25\n","313/313 [==============================] - 3s 11ms/step - loss: 0.1155 - accuracy: 0.9593 - val_loss: 0.4407 - val_accuracy: 0.8526\n","Epoch 6/25\n","313/313 [==============================] - 4s 12ms/step - loss: 0.0857 - accuracy: 0.9701 - val_loss: 0.6002 - val_accuracy: 0.8484\n","Epoch 7/25\n","313/313 [==============================] - 3s 11ms/step - loss: 0.0647 - accuracy: 0.9783 - val_loss: 0.5841 - val_accuracy: 0.8488\n","157/157 [==============================] - 1s 5ms/step - loss: 0.3158 - accuracy: 0.8634\n","GRU's Test accuracy: 0.8633999824523926\n"]}],"source":["# Tokenize the data\n","max_words = 10000\n","max_len = 100\n","tokenizer = Tokenizer(num_words=max_words)\n","tokenizer.fit_on_texts(train_data['text'])\n","\n","X_train = tokenizer.texts_to_sequences(train_data['text'])\n","X_test = tokenizer.texts_to_sequences(test_data['text'])\n","X_valid = tokenizer.texts_to_sequences(valid_data['text'])\n","\n","X_train = pad_sequences(X_train, maxlen=max_len)\n","X_test = pad_sequences(X_test, maxlen=max_len)\n","X_valid = pad_sequences(X_valid, maxlen=max_len)\n","\n","# Create the model\n","model = Sequential()\n","model.add(Embedding(max_words, 128, input_length=max_len))\n","model.add(GRU(64))\n","model.add(Dropout(0.5))\n","model.add(Dense(2, activation='softmax'))\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Set up early stopping\n","early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","# Train the model\n","history = model.fit(X_train, to_categorical(y_train), validation_data=(X_valid, to_categorical(y_valid)),\n","                    epochs=25, batch_size=128, callbacks=[early_stop])\n","\n","# Evaluate the model\n","loss, accuracy = model.evaluate(X_test, to_categorical(y_test))\n","\n","print(\"GRU's Test accuracy:\", accuracy)"]},{"cell_type":"markdown","metadata":{"id":"pMaKHe_OJpwI"},"source":["Bi Directional LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":82218,"status":"ok","timestamp":1711292395881,"user":{"displayName":"Raghavendra K","userId":"12233142523470348717"},"user_tz":420},"id":"V5v-ZgmHJsDO","outputId":"6b688499-82ad-45a6-955e-d04329a03177"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","313/313 [==============================] - 28s 75ms/step - loss: 0.3861 - accuracy: 0.8207 - val_loss: 0.3160 - val_accuracy: 0.8608\n","Epoch 2/10\n","313/313 [==============================] - 9s 27ms/step - loss: 0.2464 - accuracy: 0.8993 - val_loss: 0.3153 - val_accuracy: 0.8696\n","Epoch 3/10\n","313/313 [==============================] - 7s 22ms/step - loss: 0.1736 - accuracy: 0.9335 - val_loss: 0.3877 - val_accuracy: 0.8588\n","Epoch 4/10\n","313/313 [==============================] - 5s 17ms/step - loss: 0.1048 - accuracy: 0.9627 - val_loss: 0.5023 - val_accuracy: 0.8526\n","Epoch 5/10\n","313/313 [==============================] - 6s 18ms/step - loss: 0.0615 - accuracy: 0.9786 - val_loss: 0.5763 - val_accuracy: 0.8418\n","Epoch 6/10\n","313/313 [==============================] - 5s 16ms/step - loss: 0.0373 - accuracy: 0.9884 - val_loss: 0.6954 - val_accuracy: 0.8434\n","Epoch 7/10\n","313/313 [==============================] - 5s 16ms/step - loss: 0.0248 - accuracy: 0.9920 - val_loss: 0.8014 - val_accuracy: 0.8392\n","157/157 [==============================] - 1s 7ms/step - loss: 0.3085 - accuracy: 0.8698\n","Bi directional LSTM's Test accuracy: 0.8697999715805054\n"]}],"source":["# Tokenize the data\n","max_words = 10000\n","max_len = 100\n","tokenizer = Tokenizer(num_words=max_words)\n","tokenizer.fit_on_texts(train_data['text'])\n","\n","X_train = tokenizer.texts_to_sequences(train_data['text'])\n","X_test = tokenizer.texts_to_sequences(test_data['text'])\n","X_valid = tokenizer.texts_to_sequences(valid_data['text'])\n","\n","X_train = pad_sequences(X_train, maxlen=max_len)\n","X_test = pad_sequences(X_test, maxlen=max_len)\n","X_valid = pad_sequences(X_valid, maxlen=max_len)\n","\n","y_train = to_categorical(y_train)\n","y_test = to_categorical(y_test)\n","y_valid = to_categorical(y_valid)\n","\n","# Build the model\n","model = Sequential()\n","model.add(Embedding(max_words, 128, input_length=max_len))\n","model.add(Bidirectional(LSTM(64)))\n","model.add(Dense(2, activation='softmax'))\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Set up early stopping\n","early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","# Train the model\n","history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=10, batch_size=128, callbacks=[early_stop])\n","\n","# Evaluate the model\n","loss, accuracy = model.evaluate(X_test, y_test)\n","\n","print(\"Bi directional LSTM's Test accuracy:\", accuracy)"]},{"cell_type":"markdown","metadata":{"id":"n-pkvfvQOZnW"},"source":["### 5. Tranformers (Pre-Trained DistilBERT and RoBERTa Models)"]},{"cell_type":"markdown","metadata":{"id":"C4GQ7a_lP6ya"},"source":["DistilBERT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KB79Coth4GUv"},"outputs":[],"source":["import transformers\n","from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n","from transformers import InputExample, InputFeatures"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"klqIhy7f-_wD"},"outputs":[],"source":["\n","tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n","\n","# Tokenize the texts\n","train_encodings = tokenizer(train_data[\"text\"].tolist(), truncation=True, padding=True)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1711292745724,"user":{"displayName":"Raghavendra K","userId":"12233142523470348717"},"user_tz":420},"id":"kiOstE7u_dXV","outputId":"1a918f54-0ba9-4f98-ca26-32de342166f9"},"outputs":[{"data":{"text/plain":["[Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n"," Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["train_encodings[:2]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wOuUmq4eBqjc"},"outputs":[],"source":["test_encodings = tokenizer(test_data[\"text\"].tolist(), truncation=True, padding=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Okj71RgHAoV3"},"outputs":[],"source":["# Convert the labels to TensorFlow datasets\n","train_dataset_tf = tf.data.Dataset.from_tensor_slices((\n","    dict(train_encodings),\n","    train_data[\"label\"]\n","))\n","test_dataset_tf = tf.data.Dataset.from_tensor_slices((\n","    dict(test_encodings),\n","    test_data[\"label\"]\n","))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":158,"referenced_widgets":["8ef62d1b7e204ac099200a18bca87f95","beb31254b66d4d2883b3310e46d536dc","54eba3d78a264942bb52f5f3f6999c3c","581ac6b127cf4e64995a02bb7f89aab8","94e1dec4bd5648448c6efcb3f9858e38","4fa38a838da24d08b6ab9622c9a4b06e","99b89737c9154d5cb8a0784cb80a61cf","01f9b394067d4f7dab9ab8c749fd9da0","a8634800afec43c6a0a81b82e47f1626","59470faa44c74e4994f6fbf916525194","dd610c402d23439f8d75bc192ff21684"]},"executionInfo":{"elapsed":18829,"status":"ok","timestamp":1711293457394,"user":{"displayName":"Raghavendra K","userId":"12233142523470348717"},"user_tz":420},"id":"VQPr3D3MA7mM","outputId":"c23e0497-2e34-4096-80d7-7875d0c00964"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8ef62d1b7e204ac099200a18bca87f95","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["model = TFAutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":343,"status":"ok","timestamp":1711293540268,"user":{"displayName":"Raghavendra K","userId":"12233142523470348717"},"user_tz":420},"id":"bYZ6GcmjCaN7","outputId":"2569f09e-b2bc-435a-a3b5-e00182e8c516"},"outputs":[{"name":"stdout","output_type":"stream","text":["Adam\n"]}],"source":["# # Compile the model\n","# model.compile(optimizer=Adam,\n","#               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","#               metrics=tf.metrics.SparseCategoricalAccuracy())\n","\n","optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n","print(optimizer.name)\n","\n","# Compile the model with optimizer and loss function\n","model.compile(optimizer=optimizer.name,\n","              loss='sparse_categorical_crossentropy', metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CCaVLzzeFiuk"},"outputs":[],"source":["num_train_examples = len(train_dataset_tf)\n","num_train_batches = num_train_examples // 16\n","num_train_batches\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":11551,"status":"error","timestamp":1711294796124,"user":{"displayName":"Raghavendra K","userId":"12233142523470348717"},"user_tz":420},"id":"FOL6SKqJCzY8","outputId":"f752add9-c3d2-4fcb-a80f-329739ca290a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/2\n"]},{"ename":"ResourceExhaustedError","evalue":"Graph execution error:\n\nDetected at node gradient_tape/tf_distil_bert_for_sequence_classification/distilbert/transformer/layer_._5/ffn/Gelu/mul_1/Mul defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-27-49d55cc36d6a>\", line 2, in <cell line: 2>\n\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_tf_utils.py\", line 1161, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py\", line 1804, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py\", line 1398, in train_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py\", line 1381, in step_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py\", line 1370, in run_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_tf_utils.py\", line 1641, in train_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/optimizers/optimizer.py\", line 543, in minimize\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/optimizers/optimizer.py\", line 276, in compute_gradients\n\nfailed to allocate memory\n\t [[{{node gradient_tape/tf_distil_bert_for_sequence_classification/distilbert/transformer/layer_._5/ffn/Gelu/mul_1/Mul}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_33765]","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-816cc505a3b3>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset_tf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_batch_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node gradient_tape/tf_distil_bert_for_sequence_classification/distilbert/transformer/layer_._5/ffn/Gelu/mul_1/Mul defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-27-49d55cc36d6a>\", line 2, in <cell line: 2>\n\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_tf_utils.py\", line 1161, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py\", line 1804, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py\", line 1398, in train_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py\", line 1381, in step_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py\", line 1370, in run_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_tf_utils.py\", line 1641, in train_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/optimizers/optimizer.py\", line 543, in minimize\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/optimizers/optimizer.py\", line 276, in compute_gradients\n\nfailed to allocate memory\n\t [[{{node gradient_tape/tf_distil_bert_for_sequence_classification/distilbert/transformer/layer_._5/ffn/Gelu/mul_1/Mul}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_33765]"]}],"source":["# Train the model\n","model.fit(train_dataset_tf.shuffle(100).batch(32), epochs=2, batch_size=32)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RxEJwhUN4IPd"},"outputs":[],"source":["model_checkpoint = \"distilbert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","\n","# Prepare dataset for DistilBERT\n","train_examples = []\n","for i, row in train_data.iterrows():\n","    train_examples.append(\n","        InputExample(guid=None,\n","                     text_a=row[\"text\"],\n","                     text_b=None,\n","                     label=row[\"label\"]))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":341},"executionInfo":{"elapsed":383,"status":"error","timestamp":1711257433049,"user":{"displayName":"Raghavendra K","userId":"12233142523470348717"},"user_tz":420},"id":"EvV_I89K4nK3","outputId":"f1ddc060-a7a8-40ae-a9bf-aa91fe744a53"},"outputs":[{"ename":"TypeError","evalue":"InputFeatures.__init__() got an unexpected keyword argument 'input_mask'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-b3921741dc6d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Convert to features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m train_features = [InputFeatures(input_ids=tokenizer.encode(example.text_a, add_special_tokens=True),\n\u001b[0m\u001b[1;32m      3\u001b[0m                                  input_mask=tokenizer.encode(example.text_a, add_special_tokens=True,\n\u001b[1;32m      4\u001b[0m                                                                 \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max_length'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                                 truncation=True),\n","\u001b[0;32m<ipython-input-21-b3921741dc6d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Convert to features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m train_features = [InputFeatures(input_ids=tokenizer.encode(example.text_a, add_special_tokens=True),\n\u001b[0m\u001b[1;32m      3\u001b[0m                                  input_mask=tokenizer.encode(example.text_a, add_special_tokens=True,\n\u001b[1;32m      4\u001b[0m                                                                 \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max_length'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                                 truncation=True),\n","\u001b[0;31mTypeError\u001b[0m: InputFeatures.__init__() got an unexpected keyword argument 'input_mask'"]}],"source":["# Convert to features\n","train_features = [InputFeatures(input_ids=tokenizer.encode(example.text_a, add_special_tokens=True),\n","                                 input_mask=tokenizer.encode(example.text_a, add_special_tokens=True,\n","                                                                padding='max_length', max_length=512,\n","                                                                truncation=True),\n","                                 segment_ids=tokenizer.encode(example.text_a, add_special_tokens=True,\n","                                                               padding='max_length', max_length=512,\n","                                                               truncation=True),\n","                                 label=example.label) for example in train_examples]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SSU3NNoY3wmZ"},"outputs":[],"source":["## Use distilbert to build classifcation model\n","\n","!pip install transformers\n","import transformers\n","from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n","from transformers import InputExample, InputFeatures\n","\n","model_checkpoint = \"distilbert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","\n","# Prepare dataset for DistilBERT\n","train_examples = []\n","for i, row in train_data.iterrows():\n","    train_examples.append(\n","        InputExample(guid=None,\n","                     text_a=row[\"text\"],\n","                     text_b=None,\n","                     label=row[\"label\"]))\n","\n","# Convert to features\n","train_features = [InputFeatures(input_ids=tokenizer.encode(example.text_a, add_special_tokens=True),\n","                                 input_mask=tokenizer.encode(example.text_a, add_special_tokens=True,\n","                                                                padding='max_length', max_length=512,\n","                                                                truncation=True),\n","                                 segment_ids=tokenizer.encode(example.text_a, add_special_tokens=True,\n","                                                               padding='max_length', max_length=512,\n","                                                               truncation=True),\n","                                 label=example.label) for example in train_examples]\n","\n","# Prepare dataset for RoBERTa\n","train_examples = []\n","for i, row in train_data.iterrows():\n","    train_examples.append(\n","        InputExample(guid=None,\n","                     text_a=row[\"text\"],\n","                     text_b=None,\n","                     label=row[\"label\"]))\n","\n","# Convert to features\n","train_features = [InputFeatures(input_ids=tokenizer.encode(example.text_a, add_special_tokens=True),\n","                                 input_mask=tokenizer.encode(example.text_a, add_special_tokens=True,\n","                                                                padding='max_length', max_length=512,\n","                                                                truncation=True),\n","                                 segment_ids=tokenizer.encode(example.text_a, add_special_tokens=True,\n","                                                               padding='max_length', max_length=512,\n","                                                               truncation=True),\n","                                 label=example.label) for example in train_examples]\n","# Create TFAutoModelForSequenceClassification for DistilBERT\n","model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Set up early stopping\n","early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","# Train the model\n","history = model.fit(train_features, train_data['label'], validation_data=(X_valid, to_categorical(y_valid)),\n","                    epochs=10, batch_size=128, callbacks=[early_stop])\n","\n","# Evaluate the model\n","loss, accuracy = model.evaluate(X_test, to_categorical(y_test))\n","\n","print(\"DistilBERT's Test accuracy:\", accuracy)\n","# RoBERTa\n","# Create TFAutoModelForSequenceClassification for RoBERTa\n","model_checkpoint = \"roberta-large\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","\n","model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Set up early stopping\n","early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","# Train the model\n","history = model.fit(train_features, train_data['label'], validation_data=(X_valid, to_categorical(y_valid)),\n","                    epochs=10, batch_size=128, callbacks=[early_stop])\n","\n","# Evaluate the model\n","loss, accuracy = model.evaluate(X_test, to_categorical(y_test))\n","\n","print(\"RoBERTa's Test accuracy:\", accuracy)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":272,"referenced_widgets":["70072a638d5e415890a785cf2e838a20","613cd260af4046a7a511c1e7597792b2","d2ada898653c40b09efcc4bc1073669c","41a17a5999ab4464b6c678d94a7cc3a3","ca1c583553624f47890698b06f128a9b","259b613c530a45deb717d552a2679a29","ecde0a43aa39463ba149f83c9b9ba643","ed8e2749a29d45ccb9a0c6fc59a71df0","47c07ec8714140e8aa148da7a8735489","e707cf4caf2f485c8ccd5965eca97227","f0437ba1da6e43c7a4c1014e2e42fb94","2c625e369ab94929af10918b41405955","5d3a2d77f05648d79f0f630ebc4a4535","e5f49e2ceb1b4902b1ef9cdb2a3602ee","f39acc9115e04665b7d8f5aaaa66312f","a2f71113005549fd92379088c3e2d335","dbd555ac2c0d4be2b56939506326d4a8","ea41d531882d4186be767ae7e90f3502","0b5de934da554210b98517dfa80c7470","0a72d645955040a0904b5d74bb156262","e9c013a82abc4009aa4c86cbd4a72fe4","b4c3ab885d34442c8d0985429249e533","8c8e097c262c4dd9915e36d97185a848","6973976245934af2a1278e7723cc5663","8f740227a0d844cca2637208a4bb34ab","52168d80299740c1874a441ff3564ece","7236aa74f4dc4549884a23f62fa01144","2308c1bebe0b4f79bb8b6200f26e52ab","8c6317f47f48413180e4325d8c034a4b","2cdeb933666b4e9bad297dc59ab2daad","16a9d85883b64f89b8257bcc1df3b007","8d558aa74d7642459848ecd41e866c47","39f4f8e469934b299bf5761836c9734d","1f225cabea3047a9959a61ebc9ac1efa","d3dc40d676aa451c82a55b42e6057da7","be9b6456a7234f0cb155679e68e12b3c","9e60ec52cf7f42a384bea95973ad2b2a","4262e987da80492dbb32bdf170bae991","59dc05c890ea4bb180852f88503a4843","7c22d06f08db48e494b8384680dd676c","e7498fac8c6c4e82a284abb9736f7567","7d74eb6e6f4840429025016b5e879234","070119e99b524fbc8ab99089eebece4d","4634099ff2d74232bffb3306505b010f"]},"executionInfo":{"elapsed":5255,"status":"ok","timestamp":1711256960438,"user":{"displayName":"Raghavendra K","userId":"12233142523470348717"},"user_tz":420},"id":"u_WTqgvh2wnB","outputId":"930bb118-b3b0-4d43-85ec-a8f7d279c738"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"70072a638d5e415890a785cf2e838a20","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2c625e369ab94929af10918b41405955","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8c8e097c262c4dd9915e36d97185a848","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1f225cabea3047a9959a61ebc9ac1efa","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model_name = 'distilbert-base-uncased'\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42130,"status":"ok","timestamp":1711257977636,"user":{"displayName":"Raghavendra K","userId":"12233142523470348717"},"user_tz":420},"id":"5Nwolz_A3QJ0","outputId":"a498a7d9-b8cf-4a18-a821-eefabc35c11e"},"outputs":[{"data":{"text/plain":["(transformers.tokenization_utils_base.BatchEncoding, numpy.ndarray)"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["max_len = 512\n","\n","# Tokenize and prepare training data\n","X_train = tokenizer(text=train_data['text'].tolist(), add_special_tokens=True, max_length=max_len, truncation=True, padding=True, return_tensors='tf')\n","y_train = train_data['label'].values\n","\n","type(X_train), type(y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jvUmt9xd3fUo"},"outputs":[],"source":["# Split training data into train and validation sets\n","X_train_np = X_train['input_ids'].numpy()\n","X_train_np, X_val_np, y_train, y_val = train_test_split(X_train_np, y_train, test_size=0.2, random_state=42)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JCDXxEzz7p02"},"outputs":[],"source":["# Create TensorFlow datasets\n","batch_size = 32\n","train_dataset = tf.data.Dataset.from_tensor_slices((X_train_np, y_train)).batch(batch_size)\n","val_dataset = tf.data.Dataset.from_tensor_slices((X_val_np, y_val)).batch(batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":158,"referenced_widgets":["3cb44d49ef8b44a798b049684581fd6e","86051f64249b48468e518a77485f607c","4513c4ac91404caab65bac8915175b30","1b9c27a4ca3349c0bba31e8648c16ff7","9aa5d93a62a646c19295b586f6a51564","150f4c7f26e14553beaaaa647001459c","0c921360f3334af7bdcd41410bd144d7","9dcc3d355a804c558f5166df2f32fc3e","63aceffe1e10484f847b80af10a4e45a","533ba81b49074f61abadb77b8f7b5e9d","d6b84044063b4189981262d1f301cf6d"]},"executionInfo":{"elapsed":8082,"status":"ok","timestamp":1711258366340,"user":{"displayName":"Raghavendra K","userId":"12233142523470348717"},"user_tz":420},"id":"B9nBtqkm7sqH","outputId":"8217587d-0160-42ce-aba8-56b172e794e7"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3cb44d49ef8b44a798b049684581fd6e","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# Load model and compile\n","model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":480,"status":"ok","timestamp":1711258480222,"user":{"displayName":"Raghavendra K","userId":"12233142523470348717"},"user_tz":420},"id":"HtKQcCSJ8wRu","outputId":"6496fa2f-d66a-401f-91b8-4af838e4fcee"},"outputs":[{"name":"stdout","output_type":"stream","text":["Adam\n"]}],"source":["optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n","print(optimizer.name)\n","# Compile the model with optimizer and loss function\n","model.compile(optimizer=optimizer.name,\n","              loss='sparse_categorical_crossentropy', metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":57134,"status":"error","timestamp":1711258605255,"user":{"displayName":"Raghavendra K","userId":"12233142523470348717"},"user_tz":420},"id":"am9zYvBP9JtC","outputId":"7e86350a-8857-40f6-a43e-88e02809c9c8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/2\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x7c28fe3b1360> and will run it as-is.\n","Cause: for/else statement not yet supported\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"]},{"name":"stdout","output_type":"stream","text":["WARNING: AutoGraph could not transform <function infer_framework at 0x7c28fe3b1360> and will run it as-is.\n","Cause: for/else statement not yet supported\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"]},{"ename":"ResourceExhaustedError","evalue":"Graph execution error:\n\nDetected at node gradient_tape/tf_distil_bert_for_sequence_classification/distilbert/transformer/layer_._5/ffn/Gelu/mul_1/Mul defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-30-94f2ea7abdb1>\", line 4, in <cell line: 4>\n\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_tf_utils.py\", line 1161, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py\", line 1804, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py\", line 1398, in train_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py\", line 1381, in step_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py\", line 1370, in run_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_tf_utils.py\", line 1641, in train_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/optimizers/optimizer.py\", line 543, in minimize\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/optimizers/optimizer.py\", line 276, in compute_gradients\n\nfailed to allocate memory\n\t [[{{node gradient_tape/tf_distil_bert_for_sequence_classification/distilbert/transformer/layer_._5/ffn/Gelu/mul_1/Mul}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_16279]","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-94f2ea7abdb1>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_batch_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node gradient_tape/tf_distil_bert_for_sequence_classification/distilbert/transformer/layer_._5/ffn/Gelu/mul_1/Mul defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-30-94f2ea7abdb1>\", line 4, in <cell line: 4>\n\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_tf_utils.py\", line 1161, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py\", line 1804, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py\", line 1398, in train_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py\", line 1381, in step_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py\", line 1370, in run_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_tf_utils.py\", line 1641, in train_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/optimizers/optimizer.py\", line 543, in minimize\n\n  File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/optimizers/optimizer.py\", line 276, in compute_gradients\n\nfailed to allocate memory\n\t [[{{node gradient_tape/tf_distil_bert_for_sequence_classification/distilbert/transformer/layer_._5/ffn/Gelu/mul_1/Mul}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_16279]"]}],"source":["# Train the model\n","\n","# model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)\n","model.fit(train_dataset, epochs=2, validation_data=val_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JNEXN7ipP8U-","outputId":"ae70436a-e42d-4e66-8378-50de4e1238e3"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x7d90f39cfd90> and will run it as-is.\n","Cause: for/else statement not yet supported\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"]},{"name":"stdout","output_type":"stream","text":["WARNING: AutoGraph could not transform <function infer_framework at 0x7d90f39cfd90> and will run it as-is.\n","Cause: for/else statement not yet supported\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"]}],"source":["# https://medium.com/@prateekgaurav/nlp-zero-to-hero-part-3-transformer-based-models-conclusion-8191186301a9\n","\n","def infer_framework():\n","    # Define the model name and tokenizer\n","    model_name = 'distilbert-base-uncased'\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","    # Assuming train_data and test_data are defined\n","    max_len = 512\n","\n","    # Tokenize and prepare training data\n","    X_train = tokenizer(text=train_data['text'].tolist(), add_special_tokens=True, max_length=max_len, truncation=True, padding=True, return_tensors='tf')\n","    y_train = train_data['label'].values\n","\n","    # Split training data into train and validation sets\n","    X_train_np = X_train['input_ids'].numpy()\n","    X_train_np, X_val_np, y_train, y_val = train_test_split(X_train_np, y_train, test_size=0.2, random_state=42)\n","\n","    # Create TensorFlow datasets\n","    batch_size = 32\n","    train_dataset = tf.data.Dataset.from_tensor_slices((X_train_np, y_train)).batch(batch_size)\n","    val_dataset = tf.data.Dataset.from_tensor_slices((X_val_np, y_val)).batch(batch_size)\n","\n","    # Load model and compile\n","    model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","\n","    # Define optimizer\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n","\n","    # Compile the model with optimizer and loss function\n","    model.compile(optimizer=optimizer.name, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","    # Set up early stopping\n","    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n","\n","    # Suppress AutoGraph warnings\n","    with warnings.catch_warnings():\n","        warnings.simplefilter(\"ignore\")\n","        # Train the model\n","        epochs = 10\n","        history = model.fit(train_dataset, epochs=epochs, validation_data=val_dataset, callbacks=[early_stop])\n","\n","    # Test the model\n","    X_test = tokenizer(text=test_data['text'].tolist(), add_special_tokens=True, max_length=max_len, truncation=True, padding=True, return_tensors='tf')\n","    y_test = test_data['label'].values\n","\n","    test_dataset = tf.data.Dataset.from_tensor_slices((X_test['input_ids'], X_test['attention_mask'])).batch(batch_size)\n","\n","    y_pred = model.predict(test_dataset)\n","    y_pred_labels = tf.argmax(y_pred.logits, axis=1)\n","    accuracy = np.sum(y_pred_labels.numpy() == y_test) / len(y_test)\n","    print(f\"DistilBERT's Test accuracy: {accuracy:.2f}\")\n","\n","infer_framework()"]},{"cell_type":"markdown","metadata":{"id":"8Ru6pnHbQqWY"},"source":["RoBERTa"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":216},"executionInfo":{"elapsed":395,"status":"error","timestamp":1711097813074,"user":{"displayName":"Pradeepthi Mallappa","userId":"10327419075559480673"},"user_tz":420},"id":"isRcFAmwYGM9","outputId":"81ca5bee-9dba-4ab6-f854-741eb2543462"},"outputs":[{"ename":"NameError","evalue":"name 'AutoTokenizer' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-18c7a2b020f9>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Define the model name and tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'roberta-base'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Assuming train_data and test_data are defined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"]}],"source":["# Define the model name and tokenizer\n","model_name = 'roberta-base'\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","# Assuming train_data and test_data are defined\n","max_len = 512\n","\n","# Tokenize and prepare training data\n","X_train = tokenizer(text=train_data['text'].tolist(), add_special_tokens=True, max_length=max_len, truncation=True, padding=True, return_tensors='tf')\n","y_train = train_data['label'].values\n","\n","# Split training data into train and validation sets\n","X_train_np = X_train['input_ids'].numpy()\n","X_train_np, X_val_np, y_train, y_val = train_test_split(X_train_np, y_train, test_size=0.2, random_state=42)\n","\n","# Create TensorFlow datasets\n","train_dataset = tf.data.Dataset.from_tensor_slices((X_train_np, y_train)).batch(32)\n","val_dataset = tf.data.Dataset.from_tensor_slices((X_val_np, y_val)).batch(32)\n","\n","# Load model and compile\n","model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","\n","# Define optimizer\n","optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n","\n","# Compile the model with optimizer and loss function\n","model.compile(optimizer=optimizer.name, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# Set up early stopping\n","early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n","\n","# Train the model\n","epochs = 10\n","history = model.fit(train_dataset, epochs=epochs, validation_data=val_dataset, callbacks=[early_stop])\n","\n","# Test the model\n","X_test = tokenizer(text=test_data['text'].tolist(), add_special_tokens=True, max_length=max_len, truncation=True, padding=True, return_tensors='tf')\n","y_test = test_data['label'].values\n","\n","test_dataset = tf.data.Dataset.from_tensor_slices((X_test['input_ids'], X_test['attention_mask'])).batch(32)\n","\n","y_pred = model.predict(test_dataset)\n","y_pred_labels = tf.argmax(y_pred.logits, axis=1)\n","accuracy = np.sum(y_pred_labels.numpy() == y_test) / len(y_test)\n","print(f\"DistilBERT's Test accuracy: {accuracy:.2f}\")"]},{"cell_type":"markdown","metadata":{"id":"_YfsHQbulvx-"},"source":["Pytorch - BERT classification implementation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xvBMKPKtluCQ"},"outputs":[],"source":["import builtins\n","import collections\n","import os\n","import pandas as pd\n","import numpy as np\n","import tqdm\n","import time\n","\n","from sklearn.model_selection import train_test_split\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","from torch.utils.data import IterableDataset, Dataset\n","\n","from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sQu_bsRrl1HW"},"outputs":[],"source":["# Define a custom dataset class\n","class CustomTextDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_len):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = self.texts[idx]\n","        label = self.labels[idx]\n","\n","        # Tokenize the text using the tokenizer\n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,  # Add special tokens like [CLS] and [SEP]\n","            max_length=self.max_len,  # Set the maximum length of the tokenized sequence\n","            return_token_type_ids=False,  # Do not return token type IDs\n","            padding='max_length',  # Pad or truncate the sequence to the maximum length\n","            truncation=True,  # Truncate the sequence if it exceeds the maximum length\n","            return_attention_mask=True,  # Return attention mask indicating which tokens to attend to\n","            return_tensors='pt',  # Return PyTorch tensors\n","        ) #encoding returns a dictionary containing the encoded text, attention mask, and other information like token type IDs\n","\n","\n","        # Return the encoded text, attention mask, and label as a dictionary\n","        return {\n","            'text': text,  # Original text\n","            'input_ids': encoding['input_ids'].flatten(),  # Flattened tensor of input IDs\n","            'attention_mask': encoding['attention_mask'].flatten(),  # Flattened tensor of attention mask\n","            'labels': torch.tensor(label, dtype=torch.long)  # Tensor of label\n","        }\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_t7inKsemCsS"},"outputs":[],"source":["# Load the tokenizer\n","tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hcUzw__QmETr"},"outputs":[],"source":["# Load the data\n","\n","df = pd.read_csv('/content/drive/MyDrive/NLP/Train.csv').sample(frac=0.7).reset_index(drop=True) # as dataset is too large, we will use only 70% of it\n","\n","print(len(df))\n","df.head()\n","\n","texts = df['text'].tolist()\n","labels = df['label'].tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rQAJuFFDmMZ5"},"outputs":[],"source":["# Split the data\n","train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2)\n","\n","# Create datasets\n","train_dataset = CustomTextDataset(train_texts, train_labels, tokenizer, max_len=128) # 128 is the maximum length of the tokenized sequence\n","test_dataset = CustomTextDataset(test_texts, test_labels, tokenizer, max_len=128) # 128 is the maximum length of the tokenized sequence\n","\n","# check the length of the dataset and few samples\n","print(len(train_dataset))\n","print(len(test_dataset))\n","print(train_dataset[0])\n","print(test_dataset[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DHsDkYB4mOdZ"},"outputs":[],"source":["# Create data loaders\n","batch_size = 128\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size)\n","\n","# check the length of the dataloader and few samples\n","print(len(train_loader)) # length of the train loader is the number of batches which can be calculated as total number of samples divided by batch size\n","print(len(test_loader))\n","\n","for batch in train_loader:\n","    print(batch)\n","    print(type(batch), len(batch), batch.keys())\n","    print(len(batch['text']), len(batch['input_ids']), len(batch['attention_mask']), len(batch['labels']))\n","    print(batch['input_ids'].shape)\n","    #check the len of each of batch's texts\n","    print(len(batch['text'][0]), len(batch['text'][1]), len(batch['text'][2]))\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bDKNZ1dxmVRA"},"outputs":[],"source":["# Load the model\n","model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2) # 2 is the number of classes in the dataset\n","\n","# Move model to GPU or MPS if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"AwzWg_o-mYN8","outputId":"2ea8c06d-eec8-4e3e-8dbe-7530f1b5a413"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1, Batch 10: Loss = 0.5817462205886841\n","Epoch 1, Batch 20: Loss = 0.4283701479434967\n","Epoch 1, Batch 30: Loss = 0.4615080654621124\n","Epoch 1, Batch 40: Loss = 0.3449352979660034\n","Epoch 1, Batch 50: Loss = 0.32558831572532654\n","Epoch 1, Batch 60: Loss = 0.44657081365585327\n"]}],"source":["# Training settings\n","optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","n_epochs = 1\n","\n","# Train the model\n","model.train()\n","for epoch in range(n_epochs):\n","    for batch_idx, batch in enumerate(train_loader):\n","        optimizer.zero_grad()\n","        input_ids = batch['input_ids'].to(device) # Move input IDs to GPU or MPS\n","        attention_mask = batch['attention_mask'].to(device) # Move attention mask to GPU or MPS\n","        labels = batch['labels'].to(device) # Move labels to GPU or MPS\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward() # Compute the gradients\n","        optimizer.step() # Update the weights\n","        # Print the loss for every 100th batch\n","        if (batch_idx + 1) % 10 == 0:\n","            print(f'Epoch {epoch+1}, Batch {batch_idx+1}: Loss = {loss.item()}')\n","    print(f'Epoch {epoch+1} completed')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dKDkf5i9mfCl"},"outputs":[],"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score\n","# Evaluate the model\n","model.eval()\n","total = 0\n","correct = 0\n","\n","# Initialize variables\n","batch_accuracy = []\n","batch_precision = []\n","batch_recall = []\n","\n","# Iterate over each batch\n","for batch in test_loader:\n","    input_ids = batch['input_ids'].to(device)\n","    attention_mask = batch['attention_mask'].to(device)\n","    labels = batch['labels'].to(device)\n","    outputs = model(input_ids, attention_mask=attention_mask)\n","    _, predicted = torch.max(outputs.logits, 1)\n","\n","    # Calculate metrics for the batch\n","    batch_accuracy.append(accuracy_score(labels.cpu(), predicted.cpu()))\n","    batch_precision.append(precision_score(labels.cpu(), predicted.cpu()))\n","    batch_recall.append(recall_score(labels.cpu(), predicted.cpu()))\n","\n","    # Print metrics for the batch\n","    print(f\"Batch Accuracy: {batch_accuracy[-1]:.4f}\")\n","    print(f\"Batch Precision: {batch_precision[-1]:.4f}\")\n","    print(f\"Batch Recall: {batch_recall[-1]:.4f}\")\n","    print(\"------------------------\")\n","\n","# Calculate cumulative metrics\n","cumulative_accuracy = sum(batch_accuracy) / len(batch_accuracy)\n","cumulative_precision = sum(batch_precision) / len(batch_precision)\n","cumulative_recall = sum(batch_recall) / len(batch_recall)\n","\n","# Print cumulative metrics\n","print(f\"Cumulative Accuracy: {cumulative_accuracy:.4f}\")\n","print(f\"Cumulative Precision: {cumulative_precision:.4f}\")\n","print(f\"Cumulative Recall: {cumulative_recall:.4f}\")\n"]},{"cell_type":"markdown","source":["#### Simple RNN on pytorch"],"metadata":{"id":"4GSR3J1mX5z-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yw8I2KZkmjfZ"},"outputs":[],"source":["### Lets try simple RNN model\n","\n","# import data\n","df = pd.read_csv('/content/drive/MyDrive/NLP/Train.csv').sample(frac=0.7).reset_index(drop=True) # as dataset is too large, we will use only 70% of it\n","\n","print(len(df))\n","df.head()\n","\n","texts = df['text'].tolist()\n","labels = df['label'].tolist()\n","\n","# Split the data into train and validation\n","train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n","\n","# Tokenizer initialization\n","# basic english tokenizer from torchtext\n","from torchtext.data.utils import get_tokenizer\n","from collections import Counter\n","\n","tokenizer = get_tokenizer(\"basic_english\")\n","\n","# Create a vocabulary\n","counter = Counter()\n","for text in train_texts:\n","    counter.update(tokenizer(text))\n","\n","vocab = list(counter)\n","vocab = ['<unk>', '<pad>'] + vocab # Add special tokens for unknown tokens and padding\n","#vocab size\n","print(f'vocab size: {len(vocab)}')\n","\n","# Create a dictionary\n","word2idx = {word: idx for idx, word in enumerate(vocab)}\n","\n","# Define a function to convert text to a tensor\n","def text_to_tensor(text, word2idx, max_len=128):\n","    tensor = torch.zeros(max_len, dtype=torch.long) # Initialize a tensor of zeros with a maximum length\n","    tokens = tokenizer(text)\n","    for idx, token in enumerate(tokens):\n","        if idx >= max_len:\n","            break\n","        tensor[idx] = word2idx.get(token, 0) # Use the index of the token if it exists, otherwise use the index of the unknown token\n","    return tensor\n","\n","# Define a function to convert label to a tensor\n","def label_to_tensor(label):\n","    return torch.tensor(label, dtype=torch.long)\n"]},{"cell_type":"code","source":["# Define a custom dataset class\n","class CustomTextDataset_4(Dataset):\n","    def __init__(self, texts, labels, word2idx, max_len=128):\n","        self.texts = texts\n","        self.labels = labels\n","        self.word2idx = word2idx\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = self.texts[idx]\n","        label = self.labels[idx]\n","        input_ids = text_to_tensor(text, self.word2idx, self.max_len)\n","        labels = label_to_tensor(label)\n","        return {'text': text,  # Original text\n","                'input_ids': input_ids,\n","                'labels': labels}\n","\n","# Create datasets\n","train_dataset = CustomTextDataset_4(train_texts, train_labels, word2idx)\n","val_dataset = CustomTextDataset_4(val_texts, val_labels, word2idx)\n","\n","# check the length of the dataset and few samples\n","print(len(train_dataset))\n","print(len(val_dataset))\n","print(train_dataset[0])\n","print(val_dataset[0])"],"metadata":{"id":"9wNMs07JYH3f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create data loaders\n","batch_size = 128\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","# check the length of the dataloader and few samples\n","print(len(train_loader)) # length of the train loader is the number of batches which can be calculated as total number of samples divided by batch size\n","print(len(val_loader))\n","\n","for batch in train_loader:\n","    print(batch)\n","    print(type(batch), len(batch), batch.keys())\n","    print(len(batch['text']), len(batch['input_ids']), len(batch['labels']))\n","    print(batch['input_ids'].shape, batch['labels'].shape)\n","    #check the len of each of batch's texts\n","    print(len(batch['text'][0]), len(batch['text'][1]), len(batch['text'][2]))\n","    # so sequence length is\n","    print(batch['input_ids'].shape[1])\n","    break\n"],"metadata":{"id":"m3-7dnkOYJ1P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the RNN model\n","class RNNModel(nn.Module):\n","    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers=1, dropout=0.2):\n","        super(RNNModel, self).__init__()\n","        self.embedding = nn.Embedding(input_dim, embedding_dim)\n","        self.rnn = nn.RNN(embedding_dim, hidden_dim, n_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        # print(\"Embedding shape:\", x.shape)\n","        # [sent_len, batch_size] --> [sent_len, batch_size, emb_dim]\n","        output, hidden = self.rnn(x)\n","        # output = [sent_len, batch_size, hid_dim * n_directions]\n","        # hidden = [n_layers * n_directions, batch_size, hid_dim]\n","        hidden = hidden[-1, :, :]\n","        # [n_layers * n_directions, batch_size, hid_dim] --> [batch_size, hid_dim]\n","        return self.fc(hidden)"],"metadata":{"id":"s9NuX50jYLS_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Define the model\n","input_dim = len(vocab)\n","embedding_dim = 100\n","hidden_dim = 20\n","output_dim = 2\n","Bidirectional_flag = True\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = RNNModel(input_dim, embedding_dim, hidden_dim, output_dim)\n","model.to(device)\n","\n","\n","\n","# send a batch to the model and check the output, output dimensions\n","for batch in train_loader:\n","    input_ids = batch['input_ids'].to(device)\n","    labels = batch['labels'].to(device)\n","    print(input_ids.shape)\n","    output = model(input_ids)\n","    print(\"Output shape:\", output.shape)\n","    print(\"Output:\", output)\n","    break\n"],"metadata":{"id":"a_OxK498YMwe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training settings\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","n_epochs = 1\n","\n","# Train the model\n","model.train()\n","for epoch in range(n_epochs):\n","    for batch_idx, batch in enumerate(train_loader):\n","        optimizer.zero_grad()\n","        input_ids = batch['input_ids'].to(device) # Move input IDs to GPU or MPS\n","        labels = batch['labels'].to(device) # Move labels to GPU or MPS\n","        outputs = model(input_ids)\n","        loss = criterion(outputs, labels)\n","        loss.backward() # Compute the gradients\n","        optimizer.step() # Update the weights\n","        # Print the loss for every 100th batch\n","        if (batch_idx + 1) % 100 == 0:\n","            print(f'Epoch {epoch+1}, Batch {batch_idx+1}: Loss = {loss.item()}')\n","    print(f'Epoch {epoch+1} completed')"],"metadata":{"id":"ifmd8s7CYOtO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, precision_score, recall_score\n","# Evaluate the model\n","model.eval()\n","total = 0\n","correct = 0\n","\n","# Initialize variables\n","batch_accuracy = []\n","batch_precision = []\n","batch_recall = []\n","\n","# Iterate over each batch\n","for batch in val_loader:\n","    input_ids = batch['input_ids'].to(device)\n","    labels = batch['labels'].to(device)\n","    outputs = model(input_ids)\n","    _, predicted = torch.max(outputs, 1)\n","\n","    # Calculate metrics for the batch\n","    batch_accuracy.append(accuracy_score(labels.cpu(), predicted.cpu()))\n","    batch_precision.append(precision_score(labels.cpu(), predicted.cpu()))\n","    batch_recall.append(recall_score(labels.cpu(), predicted.cpu()))\n","\n","    # Print metrics for the batch\n","    print(f\"Batch Accuracy: {batch_accuracy[-1]:.4f}\")\n","    print(f\"Batch Precision: {batch_precision[-1]:.4f}\")\n","    print(f\"Batch Recall: {batch_recall[-1]:.4f}\")\n","    print(\"------------------------\")\n","\n","# Calculate cumulative metrics\n","cumulative_accuracy = sum(batch_accuracy) / len(batch_accuracy)\n","cumulative_precision = sum(batch_precision) / len(batch_precision)\n","cumulative_recall = sum(batch_recall) / len(batch_recall)\n","\n","# Print cumulative metrics\n","print(f\"Cumulative Accuracy: {cumulative_accuracy:.4f}\")\n","print(f\"Cumulative Precision: {cumulative_precision:.4f}\")\n","print(f\"Cumulative Recall: {cumulative_recall:.4f}\")\n"],"metadata":{"id":"xasEPOihYQWL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### LSTM on pytorch"],"metadata":{"id":"0eStOrw-YTIm"}},{"cell_type":"code","source":["# # Define the LSTM model\n","class LSTMModel(nn.Module):\n","    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers=1, dropout=0.2):\n","        super(LSTMModel, self).__init__()\n","        self.embedding = nn.Embedding(input_dim, embedding_dim)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        # print(\"Embedding shape:\", x.shape)\n","        # print elements of x\n","        # print(x)\n","        # [sent_len, batch_size] --> [sent_len, batch_size, emb_dim]\n","        output, (hidden, cell) = self.lstm(x)\n","        # output = [sent_len, batch_size, hid_dim * n_directions]\n","        # hidden = [n_layers * n_directions, batch_size, hid_dim]\n","        # cell = [n_layers * n_directions, batch_size, hid_dim]\n","        hidden = hidden[-1, :, :]\n","        # [n_layers * n_directions, batch_size, hid_dim] --> [batch_size, hid_dim]\n","        return self.fc(hidden)"],"metadata":{"id":"d960NIkBYSJ0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Define the model\n","input_dim = len(vocab)\n","embedding_dim = 256\n","hidden_dim = 20\n","output_dim = 2\n","Bidirectional_flag = True\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = LSTMModel(input_dim, embedding_dim, hidden_dim, output_dim)\n","model.to(device)\n","\n","\n","# send a batch to the model and check the output, output dimensions\n","for batch in train_loader:\n","    input_ids = batch['input_ids'].to(device)\n","    labels = batch['labels'].to(device)\n","    print(input_ids.shape)\n","    output = model(input_ids)\n","    print(\"Output shape:\", output.shape)\n","    print(\"Output:\", output)\n","    break"],"metadata":{"id":"kUOcySINYWPz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training settings\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","n_epochs = 1\n","\n","# Train the model\n","model.train()\n","for epoch in range(n_epochs):\n","    for batch_idx, batch in enumerate(train_loader):\n","        optimizer.zero_grad()\n","        input_ids = batch['input_ids'].to(device) # Move input IDs to GPU or MPS\n","        labels = batch['labels'].to(device) # Move labels to GPU or MPS\n","        outputs = model(input_ids)\n","        loss = criterion(outputs, labels)\n","        loss.backward() # Compute the gradients\n","        optimizer.step() # Update the weights\n","        # Print the loss for every 100th batch\n","        if (batch_idx + 1) % 100 == 0:\n","            print(f'Epoch {epoch+1}, Batch {batch_idx+1}: Loss = {loss.item()}')\n","    print(f'Epoch {epoch+1} completed')"],"metadata":{"id":"SXA7W04QYXr9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, precision_score, recall_score\n","# Evaluate the model\n","model.eval()\n","total = 0\n","correct = 0\n","\n","# Initialize variables\n","batch_accuracy = []\n","batch_precision = []\n","batch_recall = []\n","\n","# Iterate over each batch\n","for batch in val_loader:\n","    input_ids = batch['input_ids'].to(device)\n","    labels = batch['labels'].to(device)\n","    outputs = model(input_ids)\n","    _, predicted = torch.max(outputs, 1)\n","\n","    # Calculate metrics for the batch\n","    batch_accuracy.append(accuracy_score(labels.cpu(), predicted.cpu()))\n","    batch_precision.append(precision_score(labels.cpu(), predicted.cpu()))\n","    batch_recall.append(recall_score(labels.cpu(), predicted.cpu()))\n","\n","    # Print metrics for the batch\n","    print(f\"Batch Accuracy: {batch_accuracy[-1]:.4f}\")\n","    print(f\"Batch Precision: {batch_precision[-1]:.4f}\")\n","    print(f\"Batch Recall: {batch_recall[-1]:.4f}\")\n","    print(\"------------------------\")\n","\n","# Calculate cumulative metrics\n","cumulative_accuracy = sum(batch_accuracy) / len(batch_accuracy)\n","cumulative_precision = sum(batch_precision) / len(batch_precision)\n","cumulative_recall = sum(batch_recall) / len(batch_recall)\n","\n","# Print cumulative metrics\n","print(f\"Cumulative Accuracy: {cumulative_accuracy:.4f}\")\n","print(f\"Cumulative Precision: {cumulative_precision:.4f}\")\n","print(f\"Cumulative Recall: {cumulative_recall:.4f}\")\n"],"metadata":{"id":"_GvF8_sYYZOq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"j9WpXQuXYbBU"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1tTWaGemisFAja50oOFmuIzGRbnMy7SlD","timestamp":1711063101134}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"},"widgets":{"application/vnd.jupyter.widget-state+json":{"01f9b394067d4f7dab9ab8c749fd9da0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"070119e99b524fbc8ab99089eebece4d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a72d645955040a0904b5d74bb156262":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0b5de934da554210b98517dfa80c7470":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c921360f3334af7bdcd41410bd144d7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"150f4c7f26e14553beaaaa647001459c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16a9d85883b64f89b8257bcc1df3b007":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1b9c27a4ca3349c0bba31e8648c16ff7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_533ba81b49074f61abadb77b8f7b5e9d","placeholder":"","style":"IPY_MODEL_d6b84044063b4189981262d1f301cf6d","value":"268M/268M[00:05&lt;00:00,27.9MB/s]"}},"1f225cabea3047a9959a61ebc9ac1efa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d3dc40d676aa451c82a55b42e6057da7","IPY_MODEL_be9b6456a7234f0cb155679e68e12b3c","IPY_MODEL_9e60ec52cf7f42a384bea95973ad2b2a"],"layout":"IPY_MODEL_4262e987da80492dbb32bdf170bae991"}},"2308c1bebe0b4f79bb8b6200f26e52ab":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"259b613c530a45deb717d552a2679a29":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c625e369ab94929af10918b41405955":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5d3a2d77f05648d79f0f630ebc4a4535","IPY_MODEL_e5f49e2ceb1b4902b1ef9cdb2a3602ee","IPY_MODEL_f39acc9115e04665b7d8f5aaaa66312f"],"layout":"IPY_MODEL_a2f71113005549fd92379088c3e2d335"}},"2cdeb933666b4e9bad297dc59ab2daad":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39f4f8e469934b299bf5761836c9734d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3cb44d49ef8b44a798b049684581fd6e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_86051f64249b48468e518a77485f607c","IPY_MODEL_4513c4ac91404caab65bac8915175b30","IPY_MODEL_1b9c27a4ca3349c0bba31e8648c16ff7"],"layout":"IPY_MODEL_9aa5d93a62a646c19295b586f6a51564"}},"41a17a5999ab4464b6c678d94a7cc3a3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e707cf4caf2f485c8ccd5965eca97227","placeholder":"","style":"IPY_MODEL_f0437ba1da6e43c7a4c1014e2e42fb94","value":"28.0/28.0[00:00&lt;00:00,772B/s]"}},"4262e987da80492dbb32bdf170bae991":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4513c4ac91404caab65bac8915175b30":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9dcc3d355a804c558f5166df2f32fc3e","max":267954768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_63aceffe1e10484f847b80af10a4e45a","value":267954768}},"4634099ff2d74232bffb3306505b010f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"47c07ec8714140e8aa148da7a8735489":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4fa38a838da24d08b6ab9622c9a4b06e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52168d80299740c1874a441ff3564ece":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d558aa74d7642459848ecd41e866c47","placeholder":"","style":"IPY_MODEL_39f4f8e469934b299bf5761836c9734d","value":"232k/232k[00:00&lt;00:00,5.88MB/s]"}},"533ba81b49074f61abadb77b8f7b5e9d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54eba3d78a264942bb52f5f3f6999c3c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_01f9b394067d4f7dab9ab8c749fd9da0","max":267954768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a8634800afec43c6a0a81b82e47f1626","value":267954768}},"581ac6b127cf4e64995a02bb7f89aab8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_59470faa44c74e4994f6fbf916525194","placeholder":"","style":"IPY_MODEL_dd610c402d23439f8d75bc192ff21684","value":"268M/268M[00:14&lt;00:00,36.1MB/s]"}},"59470faa44c74e4994f6fbf916525194":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59dc05c890ea4bb180852f88503a4843":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d3a2d77f05648d79f0f630ebc4a4535":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dbd555ac2c0d4be2b56939506326d4a8","placeholder":"","style":"IPY_MODEL_ea41d531882d4186be767ae7e90f3502","value":"config.json:100%"}},"613cd260af4046a7a511c1e7597792b2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_259b613c530a45deb717d552a2679a29","placeholder":"","style":"IPY_MODEL_ecde0a43aa39463ba149f83c9b9ba643","value":"tokenizer_config.json:100%"}},"63aceffe1e10484f847b80af10a4e45a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6973976245934af2a1278e7723cc5663":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2308c1bebe0b4f79bb8b6200f26e52ab","placeholder":"","style":"IPY_MODEL_8c6317f47f48413180e4325d8c034a4b","value":"vocab.txt:100%"}},"70072a638d5e415890a785cf2e838a20":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_613cd260af4046a7a511c1e7597792b2","IPY_MODEL_d2ada898653c40b09efcc4bc1073669c","IPY_MODEL_41a17a5999ab4464b6c678d94a7cc3a3"],"layout":"IPY_MODEL_ca1c583553624f47890698b06f128a9b"}},"7236aa74f4dc4549884a23f62fa01144":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c22d06f08db48e494b8384680dd676c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7d74eb6e6f4840429025016b5e879234":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"86051f64249b48468e518a77485f607c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_150f4c7f26e14553beaaaa647001459c","placeholder":"","style":"IPY_MODEL_0c921360f3334af7bdcd41410bd144d7","value":"model.safetensors:100%"}},"8c6317f47f48413180e4325d8c034a4b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8c8e097c262c4dd9915e36d97185a848":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6973976245934af2a1278e7723cc5663","IPY_MODEL_8f740227a0d844cca2637208a4bb34ab","IPY_MODEL_52168d80299740c1874a441ff3564ece"],"layout":"IPY_MODEL_7236aa74f4dc4549884a23f62fa01144"}},"8d558aa74d7642459848ecd41e866c47":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ef62d1b7e204ac099200a18bca87f95":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_beb31254b66d4d2883b3310e46d536dc","IPY_MODEL_54eba3d78a264942bb52f5f3f6999c3c","IPY_MODEL_581ac6b127cf4e64995a02bb7f89aab8"],"layout":"IPY_MODEL_94e1dec4bd5648448c6efcb3f9858e38"}},"8f740227a0d844cca2637208a4bb34ab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2cdeb933666b4e9bad297dc59ab2daad","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_16a9d85883b64f89b8257bcc1df3b007","value":231508}},"94e1dec4bd5648448c6efcb3f9858e38":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99b89737c9154d5cb8a0784cb80a61cf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9aa5d93a62a646c19295b586f6a51564":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9dcc3d355a804c558f5166df2f32fc3e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e60ec52cf7f42a384bea95973ad2b2a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_070119e99b524fbc8ab99089eebece4d","placeholder":"","style":"IPY_MODEL_4634099ff2d74232bffb3306505b010f","value":"466k/466k[00:00&lt;00:00,651kB/s]"}},"a2f71113005549fd92379088c3e2d335":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8634800afec43c6a0a81b82e47f1626":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b4c3ab885d34442c8d0985429249e533":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"be9b6456a7234f0cb155679e68e12b3c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7498fac8c6c4e82a284abb9736f7567","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7d74eb6e6f4840429025016b5e879234","value":466062}},"beb31254b66d4d2883b3310e46d536dc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4fa38a838da24d08b6ab9622c9a4b06e","placeholder":"","style":"IPY_MODEL_99b89737c9154d5cb8a0784cb80a61cf","value":"model.safetensors:100%"}},"ca1c583553624f47890698b06f128a9b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2ada898653c40b09efcc4bc1073669c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed8e2749a29d45ccb9a0c6fc59a71df0","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_47c07ec8714140e8aa148da7a8735489","value":28}},"d3dc40d676aa451c82a55b42e6057da7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_59dc05c890ea4bb180852f88503a4843","placeholder":"","style":"IPY_MODEL_7c22d06f08db48e494b8384680dd676c","value":"tokenizer.json:100%"}},"d6b84044063b4189981262d1f301cf6d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dbd555ac2c0d4be2b56939506326d4a8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd610c402d23439f8d75bc192ff21684":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e5f49e2ceb1b4902b1ef9cdb2a3602ee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b5de934da554210b98517dfa80c7470","max":483,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0a72d645955040a0904b5d74bb156262","value":483}},"e707cf4caf2f485c8ccd5965eca97227":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7498fac8c6c4e82a284abb9736f7567":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9c013a82abc4009aa4c86cbd4a72fe4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea41d531882d4186be767ae7e90f3502":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ecde0a43aa39463ba149f83c9b9ba643":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ed8e2749a29d45ccb9a0c6fc59a71df0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0437ba1da6e43c7a4c1014e2e42fb94":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f39acc9115e04665b7d8f5aaaa66312f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9c013a82abc4009aa4c86cbd4a72fe4","placeholder":"","style":"IPY_MODEL_b4c3ab885d34442c8d0985429249e533","value":"483/483[00:00&lt;00:00,9.70kB/s]"}}}}},"nbformat":4,"nbformat_minor":0}