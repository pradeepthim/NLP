{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j9F-nPGPAew"
      },
      "source": [
        "# Classify the sentiments of IMDB reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cj2XLuMOPAex",
        "outputId": "2b6aafef-64d5-4122-e8e8-6e9733e53121"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCKjWTGTPAez"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from gensim.models import word2vec\n",
        "import gensim.downloader as api\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense, Embedding, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.layers import GRU, Dense, Embedding, Dropout, Bidirectional, LSTM\n",
        "import tensorflow as tf\n",
        "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
        "import warnings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MPbSLmJQ33A",
        "outputId": "cd1f9dbe-d5d5-4420-847c-cfccd564dec2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqKMZEIDPAez"
      },
      "source": [
        "### 1. Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY2BLBF9PAez"
      },
      "source": [
        "The dataset is from kaggle which is already divided into train, test and validation data set.\n",
        "\n",
        "https://www.kaggle.com/datasets/columbine/imdb-dataset-sentiment-analysis-in-csv-format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ez4z2lxcPAez"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv(\"/content/drive/MyDrive/NLP/Train.csv\")\n",
        "test_data = pd.read_csv(\"/content/drive/MyDrive/NLP/Test.csv\")\n",
        "valid_data = pd.read_csv(\"/content/drive/MyDrive/NLP/Valid.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3wA99obPAez"
      },
      "source": [
        "### 2. Data Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JssMEkQkPAez"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "\n",
        "    # Remove non-alphabetic characters and convert to lowercase\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text).lower()\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords from tokenized text -- right now i am not removing stopwords as I dont want to remove words like \"not\" from reviews\n",
        "    # stop_words = set(stopwords.words('english'))\n",
        "    # words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Lemmatize the words from tokenized text\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    # Combine words back into a single string\n",
        "    preprocessed_text = ' '.join(words)\n",
        "\n",
        "    return preprocessed_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMZJgaDLQDPs"
      },
      "outputs": [],
      "source": [
        "train_data['processed_text'] = train_data['text'].apply(preprocess_text)\n",
        "test_data['processed_text'] = test_data['text'].apply(preprocess_text)\n",
        "valid_data['processed_text'] = valid_data['text'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouRJ2bf1YUSl"
      },
      "source": [
        "### 3. Vectorization (using BoW, TF IDF , Word2Vec) with SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0nonw2QdMS1"
      },
      "outputs": [],
      "source": [
        "X_train = train_data['processed_text']\n",
        "y_train = train_data['label']\n",
        "\n",
        "X_test = test_data['processed_text']\n",
        "y_test = test_data['label']\n",
        "\n",
        "X_valid = valid_data['processed_text']\n",
        "y_valid = valid_data['label']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKK6IUj2b5rH"
      },
      "source": [
        "BoW and TF-IDF with SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zObq4n7lWQfF"
      },
      "outputs": [],
      "source": [
        "def train_svm_with_representations(train_data, test_data, representation):\n",
        "    if representation == 'bow':\n",
        "        vectorizer = CountVectorizer()\n",
        "    elif representation == 'tfidf':\n",
        "        vectorizer = TfidfVectorizer()\n",
        "    else:\n",
        "        raise ValueError(\"Choose one representation from'bow' or 'tfidf'.\")\n",
        "\n",
        "    X_train = vectorizer.fit_transform(train_data)\n",
        "    X_test = vectorizer.transform(test_data)\n",
        "\n",
        "    clf = SVC()\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HL4k4iKidxPZ"
      },
      "outputs": [],
      "source": [
        "y_pred_bow = train_svm_with_representations(X_train, X_test, 'bow')\n",
        "accuracy_bow = accuracy_score(y_test, y_pred_bow)\n",
        "print(\"Accuracy of BoW: \", accuracy_bow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4MXSuSaeGqe"
      },
      "outputs": [],
      "source": [
        "y_pred_tfidf = train_svm_with_representations(X_train, X_test, 'tfidf')\n",
        "accuracy_tfidf = accuracy_score(y_test, y_pred_tfidf)\n",
        "print(\"Accuracy of TF-IDF: \", accuracy_tfidf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ukjxc4Rb8_1"
      },
      "source": [
        "Word2Vec with SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IffdlCscKB-"
      },
      "outputs": [],
      "source": [
        "def get_word2vec_embeddings(data):\n",
        "    tokenized_sentences = [sentence.split() for sentence in data]\n",
        "    model = word2vec(tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "    embeddings = np.array([np.mean([model.wv[word] for word in sentence], axis=0) for sentence in tokenized_sentences])\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "def train_svm_with_word2vec(train_data, test_data):\n",
        "    X_train = get_word2vec_embeddings(train_data)\n",
        "    X_test = get_word2vec_embeddings(test_data)\n",
        "\n",
        "    clf = SVC()\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-Sq50kygc4U"
      },
      "source": [
        "Word2Vec (using Google News pretrained Word2Vec) with SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBNt6PJ-gcKd"
      },
      "outputs": [],
      "source": [
        "def get_google_word2vec_embeddings(data):\n",
        "    # Load the Google News Word2Vec model\n",
        "    model = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "    tokenized_sentences = [sentence.split() for sentence in data]\n",
        "    embeddings = []\n",
        "\n",
        "    for sentence in tokenized_sentences:\n",
        "        sentence_embeddings = []\n",
        "        for word in sentence:\n",
        "            if word in model:\n",
        "                sentence_embeddings.append(model[word])\n",
        "        if sentence_embeddings:\n",
        "            embeddings.append(np.mean(sentence_embeddings, axis=0))\n",
        "        else:\n",
        "            embeddings.append(np.zeros(300))\n",
        "\n",
        "    return np.array(embeddings)\n",
        "\n",
        "def train_svm_with_google_word2vec(train_data, test_data):\n",
        "    X_train = get_google_word2vec_embeddings(train_data)\n",
        "    X_test = get_google_word2vec_embeddings(test_data)\n",
        "\n",
        "    clf = SVC()\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzCIgBe0cYxe",
        "outputId": "09811244-852b-47e0-b528-dd1bb9f76639"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
            "Accuracy of word2Vec model: 0.8612\n"
          ]
        }
      ],
      "source": [
        "# Google News Word2Vec\n",
        "y_pred_google_word2vec = train_svm_with_google_word2vec(X_train, X_test)\n",
        "accuracy_google_word2vec = accuracy_score(y_test, y_pred_google_word2vec)\n",
        "print(\"Accuracy of word2Vec model:\", accuracy_google_word2vec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fa2hH_Bmjay"
      },
      "source": [
        "In addition to using this pre trained model from Google News 300, we can also train our model ourselves by choosing most optimized hyper parameters like window, vector_size, workers, min_count, etc).\n",
        "\n",
        "We can also use any other model other than SVM to see if the accuracy is better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0nTjG1VmEpC"
      },
      "source": [
        "### 4. RNN based models (Vanilla RNN, LSTM, GRU, Bi-Directional LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jfn0ARCWnnV7"
      },
      "source": [
        "Vanilla RNN (on processed text) - with early stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Bs7GcWalAE_",
        "outputId": "78a0617a-d4e3-4ef5-ad06-793a808ee45c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "313/313 [==============================] - 37s 111ms/step - loss: 0.4609 - accuracy: 0.7808 - val_loss: 0.3652 - val_accuracy: 0.8470\n",
            "Epoch 2/25\n",
            "313/313 [==============================] - 30s 96ms/step - loss: 0.3049 - accuracy: 0.8795 - val_loss: 0.3476 - val_accuracy: 0.8540\n",
            "Epoch 3/25\n",
            "313/313 [==============================] - 44s 139ms/step - loss: 0.2164 - accuracy: 0.9176 - val_loss: 0.4173 - val_accuracy: 0.8174\n",
            "Epoch 4/25\n",
            "313/313 [==============================] - 29s 92ms/step - loss: 0.1217 - accuracy: 0.9574 - val_loss: 0.5192 - val_accuracy: 0.7996\n",
            "Epoch 5/25\n",
            "313/313 [==============================] - 34s 109ms/step - loss: 0.0662 - accuracy: 0.9777 - val_loss: 0.6435 - val_accuracy: 0.8052\n",
            "Epoch 6/25\n",
            "313/313 [==============================] - 35s 112ms/step - loss: 0.0394 - accuracy: 0.9870 - val_loss: 0.6997 - val_accuracy: 0.8278\n",
            "Epoch 7/25\n",
            "313/313 [==============================] - 28s 88ms/step - loss: 0.0281 - accuracy: 0.9904 - val_loss: 0.8073 - val_accuracy: 0.8394\n",
            "Epoch 8/25\n",
            "313/313 [==============================] - 26s 83ms/step - loss: 0.0609 - accuracy: 0.9777 - val_loss: 0.7186 - val_accuracy: 0.8310\n",
            "Epoch 9/25\n",
            "313/313 [==============================] - 29s 91ms/step - loss: 0.0356 - accuracy: 0.9873 - val_loss: 0.8179 - val_accuracy: 0.8096\n",
            "Epoch 10/25\n",
            "313/313 [==============================] - 27s 85ms/step - loss: 0.0190 - accuracy: 0.9934 - val_loss: 0.9362 - val_accuracy: 0.8168\n",
            "Epoch 11/25\n",
            "313/313 [==============================] - 25s 78ms/step - loss: 0.0202 - accuracy: 0.9937 - val_loss: 1.0491 - val_accuracy: 0.7894\n",
            "Epoch 12/25\n",
            "313/313 [==============================] - 25s 80ms/step - loss: 0.0275 - accuracy: 0.9907 - val_loss: 1.0064 - val_accuracy: 0.8298\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.3539 - accuracy: 0.8480\n",
            "Vanila RNN's Test accuracy: 0.8479999899864197\n"
          ]
        }
      ],
      "source": [
        "# Tokenize the data\n",
        "max_words = 10000\n",
        "max_len = 100\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "\n",
        "## tokenize\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "X_valid = tokenizer.texts_to_sequences(X_valid)\n",
        "\n",
        "## padding\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)\n",
        "X_valid = pad_sequences(X_valid, maxlen=max_len)\n",
        "\n",
        "# Create the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, 128, input_length=max_len))\n",
        "model.add(SimpleRNN(64))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Set up early stopping\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, to_categorical(y_train), validation_data=(X_valid, to_categorical(y_valid)),\n",
        "                    epochs=25, batch_size=128, callbacks=[early_stop])\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, to_categorical(y_test))\n",
        "\n",
        "print(\"Vanila RNN's Test accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDZ4hpzpGvLh"
      },
      "source": [
        "The epoch stopped at 12 because I have included early stop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kvLK7xgELVI"
      },
      "source": [
        "Vanilla RNN (on un processed text) - with early stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bog_MvPjD_rX",
        "outputId": "58ee7a8d-efac-478b-abfa-f92ce9e4cc6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "313/313 [==============================] - 30s 89ms/step - loss: 0.4851 - accuracy: 0.7517 - val_loss: 0.4010 - val_accuracy: 0.8256\n",
            "Epoch 2/25\n",
            "313/313 [==============================] - 24s 78ms/step - loss: 0.3003 - accuracy: 0.8799 - val_loss: 0.3558 - val_accuracy: 0.8472\n",
            "Epoch 3/25\n",
            "313/313 [==============================] - 35s 112ms/step - loss: 0.1986 - accuracy: 0.9248 - val_loss: 0.4154 - val_accuracy: 0.8548\n",
            "Epoch 4/25\n",
            "313/313 [==============================] - 39s 125ms/step - loss: 0.1045 - accuracy: 0.9637 - val_loss: 0.5588 - val_accuracy: 0.8120\n",
            "Epoch 5/25\n",
            "313/313 [==============================] - 27s 87ms/step - loss: 0.0603 - accuracy: 0.9798 - val_loss: 0.6535 - val_accuracy: 0.8208\n",
            "Epoch 6/25\n",
            "313/313 [==============================] - 29s 92ms/step - loss: 0.0410 - accuracy: 0.9860 - val_loss: 0.7024 - val_accuracy: 0.8178\n",
            "Epoch 7/25\n",
            "313/313 [==============================] - 26s 83ms/step - loss: 0.0329 - accuracy: 0.9894 - val_loss: 0.7649 - val_accuracy: 0.8212\n",
            "Epoch 8/25\n",
            "313/313 [==============================] - 31s 99ms/step - loss: 0.0301 - accuracy: 0.9898 - val_loss: 0.8018 - val_accuracy: 0.8338\n",
            "Epoch 9/25\n",
            "313/313 [==============================] - 26s 84ms/step - loss: 0.0208 - accuracy: 0.9930 - val_loss: 0.8730 - val_accuracy: 0.8416\n",
            "Epoch 10/25\n",
            "313/313 [==============================] - 24s 77ms/step - loss: 0.0373 - accuracy: 0.9869 - val_loss: 0.8997 - val_accuracy: 0.8114\n",
            "Epoch 11/25\n",
            "313/313 [==============================] - 25s 81ms/step - loss: 0.0192 - accuracy: 0.9932 - val_loss: 0.9380 - val_accuracy: 0.8238\n",
            "Epoch 12/25\n",
            "313/313 [==============================] - 26s 82ms/step - loss: 0.0243 - accuracy: 0.9916 - val_loss: 1.0070 - val_accuracy: 0.8278\n",
            "157/157 [==============================] - 1s 9ms/step - loss: 0.3523 - accuracy: 0.8492\n",
            "Vanila RNN's Test accuracy for un processed text: 0.8492000102996826\n"
          ]
        }
      ],
      "source": [
        "# Tokenize the data\n",
        "max_words = 10000\n",
        "max_len = 100\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(train_data['text'])\n",
        "\n",
        "\n",
        "## tokenize\n",
        "X_train = tokenizer.texts_to_sequences(train_data['text'])\n",
        "X_test = tokenizer.texts_to_sequences(test_data['text'])\n",
        "X_valid = tokenizer.texts_to_sequences(valid_data['text'])\n",
        "\n",
        "## padding\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)\n",
        "X_valid = pad_sequences(X_valid, maxlen=max_len)\n",
        "\n",
        "# Create the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, 128, input_length=max_len))\n",
        "model.add(SimpleRNN(64))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Set up early stopping\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, to_categorical(y_train), validation_data=(X_valid, to_categorical(y_valid)),\n",
        "                    epochs=25, batch_size=128, callbacks=[early_stop])\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, to_categorical(y_test))\n",
        "\n",
        "print(\"Vanila RNN's Test accuracy for un processed text:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Uofsbm9ExoW"
      },
      "source": [
        "Vanilla RNN has vanishing gradient problem. So to remove this, we use LSTM. The accuracy of Vanilla RNN using un processed text might be more in most cases because the RNN will take care of pre process by ensuring not much information is lost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAw83OkZEnIg"
      },
      "source": [
        "LSTM (Long Short Term Memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvF6fNJ4Eq3n",
        "outputId": "c31790e8-e283-4c21-d6fd-d2bbab0a5d30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "313/313 [==============================] - 25s 68ms/step - loss: 0.3976 - accuracy: 0.8181 - val_loss: 0.3364 - val_accuracy: 0.8510\n",
            "Epoch 2/10\n",
            "313/313 [==============================] - 9s 30ms/step - loss: 0.2576 - accuracy: 0.8961 - val_loss: 0.3419 - val_accuracy: 0.8606\n",
            "Epoch 3/10\n",
            "313/313 [==============================] - 5s 17ms/step - loss: 0.2003 - accuracy: 0.9215 - val_loss: 0.3776 - val_accuracy: 0.8558\n",
            "Epoch 4/10\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.1580 - accuracy: 0.9393 - val_loss: 0.4316 - val_accuracy: 0.8528\n",
            "Epoch 5/10\n",
            "313/313 [==============================] - 4s 13ms/step - loss: 0.1283 - accuracy: 0.9522 - val_loss: 0.5002 - val_accuracy: 0.8432\n",
            "Epoch 6/10\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.1058 - accuracy: 0.9611 - val_loss: 0.5992 - val_accuracy: 0.8402\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.3320 - accuracy: 0.8528\n",
            "LSTM test accuracy: 0.8528000116348267\n"
          ]
        }
      ],
      "source": [
        "# Tokenize the data\n",
        "max_words = 10000\n",
        "max_len = 100\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(train_data['text'])\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(train_data['text'])\n",
        "X_test = tokenizer.texts_to_sequences(test_data['text'])\n",
        "X_valid = tokenizer.texts_to_sequences(valid_data['text'])\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)\n",
        "X_valid = pad_sequences(X_valid, maxlen=max_len)\n",
        "\n",
        "# Create the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, 128, input_length=max_len))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Set up early stopping\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, to_categorical(y_train), validation_data=(X_valid, to_categorical(y_valid)),\n",
        "                    epochs=10, batch_size=128, callbacks=[early_stop])\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, to_categorical(y_test))\n",
        "\n",
        "print(\"LSTM test accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2Hfacs2IIGl"
      },
      "source": [
        "Added drop out layer to avoid overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU8dHnhNIMvP"
      },
      "source": [
        "GRU Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLoCLFcqICwj",
        "outputId": "7ca12b17-cb87-456d-c66f-b114bb6081f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "313/313 [==============================] - 23s 68ms/step - loss: 0.4233 - accuracy: 0.7925 - val_loss: 0.3312 - val_accuracy: 0.8576\n",
            "Epoch 2/25\n",
            "313/313 [==============================] - 7s 23ms/step - loss: 0.2631 - accuracy: 0.8939 - val_loss: 0.3139 - val_accuracy: 0.8686\n",
            "Epoch 3/25\n",
            "313/313 [==============================] - 5s 17ms/step - loss: 0.2098 - accuracy: 0.9183 - val_loss: 0.3355 - val_accuracy: 0.8698\n",
            "Epoch 4/25\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.1586 - accuracy: 0.9406 - val_loss: 0.3726 - val_accuracy: 0.8604\n",
            "Epoch 5/25\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.1155 - accuracy: 0.9593 - val_loss: 0.4407 - val_accuracy: 0.8526\n",
            "Epoch 6/25\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0857 - accuracy: 0.9701 - val_loss: 0.6002 - val_accuracy: 0.8484\n",
            "Epoch 7/25\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.0647 - accuracy: 0.9783 - val_loss: 0.5841 - val_accuracy: 0.8488\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 0.3158 - accuracy: 0.8634\n",
            "GRU's Test accuracy: 0.8633999824523926\n"
          ]
        }
      ],
      "source": [
        "# Tokenize the data\n",
        "max_words = 10000\n",
        "max_len = 100\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(train_data['text'])\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(train_data['text'])\n",
        "X_test = tokenizer.texts_to_sequences(test_data['text'])\n",
        "X_valid = tokenizer.texts_to_sequences(valid_data['text'])\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)\n",
        "X_valid = pad_sequences(X_valid, maxlen=max_len)\n",
        "\n",
        "# Create the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, 128, input_length=max_len))\n",
        "model.add(GRU(64))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Set up early stopping\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, to_categorical(y_train), validation_data=(X_valid, to_categorical(y_valid)),\n",
        "                    epochs=25, batch_size=128, callbacks=[early_stop])\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, to_categorical(y_test))\n",
        "\n",
        "print(\"GRU's Test accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMaKHe_OJpwI"
      },
      "source": [
        "Bi Directional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5v-ZgmHJsDO",
        "outputId": "6b688499-82ad-45a6-955e-d04329a03177"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "313/313 [==============================] - 28s 75ms/step - loss: 0.3861 - accuracy: 0.8207 - val_loss: 0.3160 - val_accuracy: 0.8608\n",
            "Epoch 2/10\n",
            "313/313 [==============================] - 9s 27ms/step - loss: 0.2464 - accuracy: 0.8993 - val_loss: 0.3153 - val_accuracy: 0.8696\n",
            "Epoch 3/10\n",
            "313/313 [==============================] - 7s 22ms/step - loss: 0.1736 - accuracy: 0.9335 - val_loss: 0.3877 - val_accuracy: 0.8588\n",
            "Epoch 4/10\n",
            "313/313 [==============================] - 5s 17ms/step - loss: 0.1048 - accuracy: 0.9627 - val_loss: 0.5023 - val_accuracy: 0.8526\n",
            "Epoch 5/10\n",
            "313/313 [==============================] - 6s 18ms/step - loss: 0.0615 - accuracy: 0.9786 - val_loss: 0.5763 - val_accuracy: 0.8418\n",
            "Epoch 6/10\n",
            "313/313 [==============================] - 5s 16ms/step - loss: 0.0373 - accuracy: 0.9884 - val_loss: 0.6954 - val_accuracy: 0.8434\n",
            "Epoch 7/10\n",
            "313/313 [==============================] - 5s 16ms/step - loss: 0.0248 - accuracy: 0.9920 - val_loss: 0.8014 - val_accuracy: 0.8392\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 0.3085 - accuracy: 0.8698\n",
            "Bi directional LSTM's Test accuracy: 0.8697999715805054\n"
          ]
        }
      ],
      "source": [
        "# Tokenize the data\n",
        "max_words = 10000\n",
        "max_len = 100\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(train_data['text'])\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(train_data['text'])\n",
        "X_test = tokenizer.texts_to_sequences(test_data['text'])\n",
        "X_valid = tokenizer.texts_to_sequences(valid_data['text'])\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)\n",
        "X_valid = pad_sequences(X_valid, maxlen=max_len)\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "y_valid = to_categorical(y_valid)\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, 128, input_length=max_len))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Set up early stopping\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=10, batch_size=128, callbacks=[early_stop])\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "print(\"Bi directional LSTM's Test accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-pkvfvQOZnW"
      },
      "source": [
        "### 5. Tranformers (Pre-Trained DistilBERT Model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YfsHQbulvx-"
      },
      "source": [
        "Pytorch - BERT classification implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvBMKPKtluCQ"
      },
      "outputs": [],
      "source": [
        "import builtins\n",
        "import collections\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import time\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import IterableDataset, Dataset\n",
        "\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQu_bsRrl1HW"
      },
      "outputs": [],
      "source": [
        "# Define a custom dataset class\n",
        "class CustomTextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Tokenize the text using the tokenizer\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,  # Add special tokens like [CLS] and [SEP]\n",
        "            max_length=self.max_len,  # Set the maximum length of the tokenized sequence\n",
        "            return_token_type_ids=False,  # Do not return token type IDs\n",
        "            padding='max_length',  # Pad or truncate the sequence to the maximum length\n",
        "            truncation=True,  # Truncate the sequence if it exceeds the maximum length\n",
        "            return_attention_mask=True,  # Return attention mask indicating which tokens to attend to\n",
        "            return_tensors='pt',  # Return PyTorch tensors\n",
        "        ) #encoding returns a dictionary containing the encoded text, attention mask, and other information like token type IDs\n",
        "\n",
        "\n",
        "        # Return the encoded text, attention mask, and label as a dictionary\n",
        "        return {\n",
        "            'text': text,  # Original text\n",
        "            'input_ids': encoding['input_ids'].flatten(),  # Flattened tensor of input IDs\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),  # Flattened tensor of attention mask\n",
        "            'labels': torch.tensor(label, dtype=torch.long)  # Tensor of label\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_t7inKsemCsS"
      },
      "outputs": [],
      "source": [
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcUzw__QmETr"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/NLP/Train.csv').sample(frac=0.7).reset_index(drop=True) # as dataset is too large, we will use only 70% of it\n",
        "\n",
        "print(len(df))\n",
        "df.head()\n",
        "\n",
        "texts = df['text'].tolist()\n",
        "labels = df['label'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQAJuFFDmMZ5"
      },
      "outputs": [],
      "source": [
        "# Split the data\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = CustomTextDataset(train_texts, train_labels, tokenizer, max_len=128) # 128 is the maximum length of the tokenized sequence\n",
        "test_dataset = CustomTextDataset(test_texts, test_labels, tokenizer, max_len=128) # 128 is the maximum length of the tokenized sequence\n",
        "\n",
        "# check the length of the dataset and few samples\n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))\n",
        "print(train_dataset[0])\n",
        "print(test_dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHsDkYB4mOdZ"
      },
      "outputs": [],
      "source": [
        "# Create data loaders\n",
        "batch_size = 128\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# check the length of the dataloader and few samples\n",
        "print(len(train_loader)) # length of the train loader is the number of batches which can be calculated as total number of samples divided by batch size\n",
        "print(len(test_loader))\n",
        "\n",
        "for batch in train_loader:\n",
        "    print(batch)\n",
        "    print(type(batch), len(batch), batch.keys())\n",
        "    print(len(batch['text']), len(batch['input_ids']), len(batch['attention_mask']), len(batch['labels']))\n",
        "    print(batch['input_ids'].shape)\n",
        "    #check the len of each of batch's texts\n",
        "    print(len(batch['text'][0]), len(batch['text'][1]), len(batch['text'][2]))\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDKNZ1dxmVRA"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2) # 2 is the number of classes in the dataset\n",
        "\n",
        "# Move model to GPU or MPS if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwzWg_o-mYN8",
        "outputId": "2ea8c06d-eec8-4e3e-8dbe-7530f1b5a413"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Batch 10: Loss = 0.5817462205886841\n",
            "Epoch 1, Batch 20: Loss = 0.4283701479434967\n",
            "Epoch 1, Batch 30: Loss = 0.4615080654621124\n",
            "Epoch 1, Batch 40: Loss = 0.3449352979660034\n",
            "Epoch 1, Batch 50: Loss = 0.32558831572532654\n",
            "Epoch 1, Batch 60: Loss = 0.44657081365585327\n"
          ]
        }
      ],
      "source": [
        "# Training settings\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "n_epochs = 1\n",
        "\n",
        "# Train the model\n",
        "model.train()\n",
        "for epoch in range(n_epochs):\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device) # Move input IDs to GPU or MPS\n",
        "        attention_mask = batch['attention_mask'].to(device) # Move attention mask to GPU or MPS\n",
        "        labels = batch['labels'].to(device) # Move labels to GPU or MPS\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward() # Compute the gradients\n",
        "        optimizer.step() # Update the weights\n",
        "        # Print the loss for every 100th batch\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            print(f'Epoch {epoch+1}, Batch {batch_idx+1}: Loss = {loss.item()}')\n",
        "    print(f'Epoch {epoch+1} completed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKDkf5i9mfCl"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "total = 0\n",
        "correct = 0\n",
        "\n",
        "# Initialize variables\n",
        "batch_accuracy = []\n",
        "batch_precision = []\n",
        "batch_recall = []\n",
        "\n",
        "# Iterate over each batch\n",
        "for batch in test_loader:\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "    outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    _, predicted = torch.max(outputs.logits, 1)\n",
        "\n",
        "    # Calculate metrics for the batch\n",
        "    batch_accuracy.append(accuracy_score(labels.cpu(), predicted.cpu()))\n",
        "    batch_precision.append(precision_score(labels.cpu(), predicted.cpu()))\n",
        "    batch_recall.append(recall_score(labels.cpu(), predicted.cpu()))\n",
        "\n",
        "    # Print metrics for the batch\n",
        "    print(f\"Batch Accuracy: {batch_accuracy[-1]:.4f}\")\n",
        "    print(f\"Batch Precision: {batch_precision[-1]:.4f}\")\n",
        "    print(f\"Batch Recall: {batch_recall[-1]:.4f}\")\n",
        "    print(\"------------------------\")\n",
        "\n",
        "# Calculate cumulative metrics\n",
        "cumulative_accuracy = sum(batch_accuracy) / len(batch_accuracy)\n",
        "cumulative_precision = sum(batch_precision) / len(batch_precision)\n",
        "cumulative_recall = sum(batch_recall) / len(batch_recall)\n",
        "\n",
        "# Print cumulative metrics\n",
        "print(f\"Cumulative Accuracy: {cumulative_accuracy:.4f}\")\n",
        "print(f\"Cumulative Precision: {cumulative_precision:.4f}\")\n",
        "print(f\"Cumulative Recall: {cumulative_recall:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Additional PyTorch Based models"
      ],
      "metadata": {
        "id": "mbjLaKs3XUwr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Simple RNN on pytorch"
      ],
      "metadata": {
        "id": "4GSR3J1mX5z-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yw8I2KZkmjfZ"
      },
      "outputs": [],
      "source": [
        "### Lets try simple RNN model\n",
        "\n",
        "# import data\n",
        "df = pd.read_csv('/content/drive/MyDrive/NLP/Train.csv').sample(frac=0.7).reset_index(drop=True) # as dataset is too large, we will use only 70% of it\n",
        "\n",
        "print(len(df))\n",
        "df.head()\n",
        "\n",
        "texts = df['text'].tolist()\n",
        "labels = df['label'].tolist()\n",
        "\n",
        "# Split the data into train and validation\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n",
        "\n",
        "# Tokenizer initialization\n",
        "# basic english tokenizer from torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from collections import Counter\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# Create a vocabulary\n",
        "counter = Counter()\n",
        "for text in train_texts:\n",
        "    counter.update(tokenizer(text))\n",
        "\n",
        "vocab = list(counter)\n",
        "vocab = ['<unk>', '<pad>'] + vocab # Add special tokens for unknown tokens and padding\n",
        "#vocab size\n",
        "print(f'vocab size: {len(vocab)}')\n",
        "\n",
        "# Create a dictionary\n",
        "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "\n",
        "# Define a function to convert text to a tensor\n",
        "def text_to_tensor(text, word2idx, max_len=128):\n",
        "    tensor = torch.zeros(max_len, dtype=torch.long) # Initialize a tensor of zeros with a maximum length\n",
        "    tokens = tokenizer(text)\n",
        "    for idx, token in enumerate(tokens):\n",
        "        if idx >= max_len:\n",
        "            break\n",
        "        tensor[idx] = word2idx.get(token, 0) # Use the index of the token if it exists, otherwise use the index of the unknown token\n",
        "    return tensor\n",
        "\n",
        "# Define a function to convert label to a tensor\n",
        "def label_to_tensor(label):\n",
        "    return torch.tensor(label, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom dataset class\n",
        "class CustomTextDataset_4(Dataset):\n",
        "    def __init__(self, texts, labels, word2idx, max_len=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.word2idx = word2idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        input_ids = text_to_tensor(text, self.word2idx, self.max_len)\n",
        "        labels = label_to_tensor(label)\n",
        "        return {'text': text,  # Original text\n",
        "                'input_ids': input_ids,\n",
        "                'labels': labels}\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = CustomTextDataset_4(train_texts, train_labels, word2idx)\n",
        "val_dataset = CustomTextDataset_4(val_texts, val_labels, word2idx)\n",
        "\n",
        "# check the length of the dataset and few samples\n",
        "print(len(train_dataset))\n",
        "print(len(val_dataset))\n",
        "print(train_dataset[0])\n",
        "print(val_dataset[0])"
      ],
      "metadata": {
        "id": "9wNMs07JYH3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders\n",
        "batch_size = 128\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "# check the length of the dataloader and few samples\n",
        "print(len(train_loader)) # length of the train loader is the number of batches which can be calculated as total number of samples divided by batch size\n",
        "print(len(val_loader))\n",
        "\n",
        "for batch in train_loader:\n",
        "    print(batch)\n",
        "    print(type(batch), len(batch), batch.keys())\n",
        "    print(len(batch['text']), len(batch['input_ids']), len(batch['labels']))\n",
        "    print(batch['input_ids'].shape, batch['labels'].shape)\n",
        "    #check the len of each of batch's texts\n",
        "    print(len(batch['text'][0]), len(batch['text'][1]), len(batch['text'][2]))\n",
        "    # so sequence length is\n",
        "    print(batch['input_ids'].shape[1])\n",
        "    break\n"
      ],
      "metadata": {
        "id": "m3-7dnkOYJ1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the RNN model\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers=1, dropout=0.2):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, n_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        # print(\"Embedding shape:\", x.shape)\n",
        "        # [sent_len, batch_size] --> [sent_len, batch_size, emb_dim]\n",
        "        output, hidden = self.rnn(x)\n",
        "        # output = [sent_len, batch_size, hid_dim * n_directions]\n",
        "        # hidden = [n_layers * n_directions, batch_size, hid_dim]\n",
        "        hidden = hidden[-1, :, :]\n",
        "        # [n_layers * n_directions, batch_size, hid_dim] --> [batch_size, hid_dim]\n",
        "        return self.fc(hidden)"
      ],
      "metadata": {
        "id": "s9NuX50jYLS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the model\n",
        "input_dim = len(vocab)\n",
        "embedding_dim = 100\n",
        "hidden_dim = 20\n",
        "output_dim = 2\n",
        "Bidirectional_flag = True\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = RNNModel(input_dim, embedding_dim, hidden_dim, output_dim)\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "# send a batch to the model and check the output, output dimensions\n",
        "for batch in train_loader:\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "    print(input_ids.shape)\n",
        "    output = model(input_ids)\n",
        "    print(\"Output shape:\", output.shape)\n",
        "    print(\"Output:\", output)\n",
        "    break\n"
      ],
      "metadata": {
        "id": "a_OxK498YMwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training settings\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "n_epochs = 1\n",
        "\n",
        "# Train the model\n",
        "model.train()\n",
        "for epoch in range(n_epochs):\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device) # Move input IDs to GPU or MPS\n",
        "        labels = batch['labels'].to(device) # Move labels to GPU or MPS\n",
        "        outputs = model(input_ids)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward() # Compute the gradients\n",
        "        optimizer.step() # Update the weights\n",
        "        # Print the loss for every 100th batch\n",
        "        if (batch_idx + 1) % 100 == 0:\n",
        "            print(f'Epoch {epoch+1}, Batch {batch_idx+1}: Loss = {loss.item()}')\n",
        "    print(f'Epoch {epoch+1} completed')"
      ],
      "metadata": {
        "id": "ifmd8s7CYOtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "total = 0\n",
        "correct = 0\n",
        "\n",
        "# Initialize variables\n",
        "batch_accuracy = []\n",
        "batch_precision = []\n",
        "batch_recall = []\n",
        "\n",
        "# Iterate over each batch\n",
        "for batch in val_loader:\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "    outputs = model(input_ids)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "    # Calculate metrics for the batch\n",
        "    batch_accuracy.append(accuracy_score(labels.cpu(), predicted.cpu()))\n",
        "    batch_precision.append(precision_score(labels.cpu(), predicted.cpu()))\n",
        "    batch_recall.append(recall_score(labels.cpu(), predicted.cpu()))\n",
        "\n",
        "    # Print metrics for the batch\n",
        "    print(f\"Batch Accuracy: {batch_accuracy[-1]:.4f}\")\n",
        "    print(f\"Batch Precision: {batch_precision[-1]:.4f}\")\n",
        "    print(f\"Batch Recall: {batch_recall[-1]:.4f}\")\n",
        "    print(\"------------------------\")\n",
        "\n",
        "# Calculate cumulative metrics\n",
        "cumulative_accuracy = sum(batch_accuracy) / len(batch_accuracy)\n",
        "cumulative_precision = sum(batch_precision) / len(batch_precision)\n",
        "cumulative_recall = sum(batch_recall) / len(batch_recall)\n",
        "\n",
        "# Print cumulative metrics\n",
        "print(f\"Cumulative Accuracy: {cumulative_accuracy:.4f}\")\n",
        "print(f\"Cumulative Precision: {cumulative_precision:.4f}\")\n",
        "print(f\"Cumulative Recall: {cumulative_recall:.4f}\")\n"
      ],
      "metadata": {
        "id": "xasEPOihYQWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM on pytorch"
      ],
      "metadata": {
        "id": "0eStOrw-YTIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define the LSTM model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers=1, dropout=0.2):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        # print(\"Embedding shape:\", x.shape)\n",
        "        # print elements of x\n",
        "        # print(x)\n",
        "        # [sent_len, batch_size] --> [sent_len, batch_size, emb_dim]\n",
        "        output, (hidden, cell) = self.lstm(x)\n",
        "        # output = [sent_len, batch_size, hid_dim * n_directions]\n",
        "        # hidden = [n_layers * n_directions, batch_size, hid_dim]\n",
        "        # cell = [n_layers * n_directions, batch_size, hid_dim]\n",
        "        hidden = hidden[-1, :, :]\n",
        "        # [n_layers * n_directions, batch_size, hid_dim] --> [batch_size, hid_dim]\n",
        "        return self.fc(hidden)"
      ],
      "metadata": {
        "id": "d960NIkBYSJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the model\n",
        "input_dim = len(vocab)\n",
        "embedding_dim = 256\n",
        "hidden_dim = 20\n",
        "output_dim = 2\n",
        "Bidirectional_flag = True\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = LSTMModel(input_dim, embedding_dim, hidden_dim, output_dim)\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# send a batch to the model and check the output, output dimensions\n",
        "for batch in train_loader:\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "    print(input_ids.shape)\n",
        "    output = model(input_ids)\n",
        "    print(\"Output shape:\", output.shape)\n",
        "    print(\"Output:\", output)\n",
        "    break"
      ],
      "metadata": {
        "id": "kUOcySINYWPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training settings\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "n_epochs = 1\n",
        "\n",
        "# Train the model\n",
        "model.train()\n",
        "for epoch in range(n_epochs):\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device) # Move input IDs to GPU or MPS\n",
        "        labels = batch['labels'].to(device) # Move labels to GPU or MPS\n",
        "        outputs = model(input_ids)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward() # Compute the gradients\n",
        "        optimizer.step() # Update the weights\n",
        "        # Print the loss for every 100th batch\n",
        "        if (batch_idx + 1) % 100 == 0:\n",
        "            print(f'Epoch {epoch+1}, Batch {batch_idx+1}: Loss = {loss.item()}')\n",
        "    print(f'Epoch {epoch+1} completed')"
      ],
      "metadata": {
        "id": "SXA7W04QYXr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "total = 0\n",
        "correct = 0\n",
        "\n",
        "# Initialize variables\n",
        "batch_accuracy = []\n",
        "batch_precision = []\n",
        "batch_recall = []\n",
        "\n",
        "# Iterate over each batch\n",
        "for batch in val_loader:\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "    outputs = model(input_ids)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "    # Calculate metrics for the batch\n",
        "    batch_accuracy.append(accuracy_score(labels.cpu(), predicted.cpu()))\n",
        "    batch_precision.append(precision_score(labels.cpu(), predicted.cpu()))\n",
        "    batch_recall.append(recall_score(labels.cpu(), predicted.cpu()))\n",
        "\n",
        "    # Print metrics for the batch\n",
        "    print(f\"Batch Accuracy: {batch_accuracy[-1]:.4f}\")\n",
        "    print(f\"Batch Precision: {batch_precision[-1]:.4f}\")\n",
        "    print(f\"Batch Recall: {batch_recall[-1]:.4f}\")\n",
        "    print(\"------------------------\")\n",
        "\n",
        "# Calculate cumulative metrics\n",
        "cumulative_accuracy = sum(batch_accuracy) / len(batch_accuracy)\n",
        "cumulative_precision = sum(batch_precision) / len(batch_precision)\n",
        "cumulative_recall = sum(batch_recall) / len(batch_recall)\n",
        "\n",
        "# Print cumulative metrics\n",
        "print(f\"Cumulative Accuracy: {cumulative_accuracy:.4f}\")\n",
        "print(f\"Cumulative Precision: {cumulative_precision:.4f}\")\n",
        "print(f\"Cumulative Recall: {cumulative_recall:.4f}\")\n"
      ],
      "metadata": {
        "id": "_GvF8_sYYZOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j9WpXQuXYbBU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}